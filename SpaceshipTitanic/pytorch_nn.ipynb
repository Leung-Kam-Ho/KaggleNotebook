{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (2.1.0)\n","Requirement already satisfied: tensorboardX in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (2.6.2.2)\n","Requirement already satisfied: tensorboard in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (2.15.1)\n","Requirement already satisfied: pandas in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (2.1.2)\n","Requirement already satisfied: filelock in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from torch) (4.8.0)\n","Requirement already satisfied: sympy in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from torch) (2023.10.0)\n","Requirement already satisfied: numpy in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboardX) (1.26.1)\n","Requirement already satisfied: packaging in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboardX) (23.2)\n","Requirement already satisfied: protobuf>=3.20 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboardX) (4.23.4)\n","Requirement already satisfied: absl-py>=0.4 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (2.0.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (1.59.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (2.23.4)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (1.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (65.5.0)\n","Requirement already satisfied: six>1.9 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from tensorboard) (3.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from pandas) (2023.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /Users/fx7707/.Kaggle/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install torch tensorboardX tensorboard pandas"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from tensorboardX import SummaryWriter"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","class CustomDataSet(Dataset):\n","    def __init__(self, x,y):\n","        self.data = x\n","        self.label = y\n","        \n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, index):\n","        data = self.data[index]\n","        label = self.label[index]\n","\n","        return data, label"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class Net(nn.Module):\n","  def __init__(self,input_shape,layerWidth=512):\n","    super(Net,self).__init__()\n","    self.fc1 = nn.Linear(input_shape,layerWidth)\n","    self.fc2 = nn.Linear(layerWidth,layerWidth)\n","    self.fc3 = nn.Linear(layerWidth,layerWidth)\n","    self.fc4 = nn.Linear(layerWidth,layerWidth)\n","    self.fc5 = nn.Linear(layerWidth,1)\n","  def forward(self,x):\n","    x = torch.relu(self.fc1(x))\n","    x = torch.relu(self.fc2(x))\n","    x = torch.relu(self.fc3(x))\n","    x = torch.relu(self.fc4(x))\n","    x = torch.sigmoid(self.fc5(x))\n","    return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(8693, 23)\n"]}],"source":["csv_file_path = \"./dataset/train_denoised.csv\"\n","df = pd.read_csv(csv_file_path)\n","y = df[\"Transported\"]\n","df = df.drop(\"Transported\", axis=1)\n","x = df.values\n","\n","print(x.shape)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Lets create an object from our custom dataset class\n","train_data_object = CustomDataSet(x,y)\n","\n","# Now lets use Data loader to load the data in batches\n","train_loader = torch.utils.data.DataLoader(\n","        train_data_object,\n","        batch_size=1024,\n","        shuffle=False\n","    )"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# train_features, train_labels = next(iter(train_loader))\n","# print(f\"Feature batch shape: {train_features.size()}\")\n","# print(f\"Labels batch shape: {train_labels.size()}\")\n","# # print(train_features,train_labels)\n","# testModel = Net(20)\n","# train_features = train_features.to(torch.float32)\n","# output = testModel(train_features)\n","# output = torch.round(output)\n","# print(output)\n","# print(train_labels)\n","# print(output.eq(train_labels.data.view_as(output)).cpu().sum())"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["model = Net(x.shape[1])\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","loss_fn = nn.BCELoss()\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def train(model,epochs = 10, model_path = \"./model.pth\",log_path = \"./log_pytorch/\"):\n","    tb = SummaryWriter(log_path)\n","\n","    # put the model into training mode\n","    model.train()\n","    for epoch in range(1, epochs + 1):\n","        correct = 0\n","\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            \n","            data = data.to(torch.float32)\n","            # print(\"data\",data)\n","            target = target.to(torch.float32)\n","            target = target.unsqueeze(1)\n","            #calculate output\n","            output = model(data)\n","            # print(output)\n","            #calculate loss\n","            loss = loss_fn(output,target)\n","        \n","            #accuracy\n","            predicted = model(torch.tensor(x,dtype=torch.float32))\n","            acc = (predicted.reshape(-1).detach().numpy().round() == y).mean()\n","            #backprop\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch_idx % 100 == 0:\n","                # losses.append(loss)\n","                # accur.append(acc)\n","                print(\"epoch {}\\tloss : {}\\tAcc: {}\".format(epoch,loss,acc))\n","\n","        tb.add_scalar(\"epoch loss\", loss.item(), epoch)\n","        tb.add_scalar(\"epoch accuracy\", acc, epoch)\n","        for name, weight in model.named_parameters():\n","            tb.add_histogram(name, weight, epoch)\n","            tb.add_histogram(f'{name}.grad',weight.grad, epoch)\n","    # save the model to a .pth file\n","    print('Saving NN to %s' % model_path)\n","    torch.save(model.state_dict(), model_path)\n","    # add graph to tensorboard\n","    tb.add_graph(model, (data,))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def test(model, model_path=\"./model.pth\",testDataPath = \"./dataset/test_preprocessed\"):\n","\n","    df = pd.read_csv(testDataPath)\n","    y = df[\"PassengerId\"]\n","    df = df.drop(\"PassengerId\", axis=1)\n","    x = df.values\n","\n","    model.load_state_dict(torch.load(model_path))\n","    \n","    # put model into test mode\n","    model.eval()\n","    with torch.no_grad():\n","    \n","        output = model(torch.tensor(x,dtype=torch.float32))\n","        pred = output.reshape(-1).detach().numpy().round()\n","        label = y.values\n","        \n","        result = {\"PassengerId\" : label,\n","                  \"Transported\" : pred}\n","        \n","        return result"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["epoch 1\tloss : 0.6934706568717957\tAcc: 0.5036236051995858\n","epoch 2\tloss : 0.6934455633163452\tAcc: 0.5036236051995858\n","epoch 3\tloss : 0.6934204697608948\tAcc: 0.5036236051995858\n","epoch 4\tloss : 0.6933956146240234\tAcc: 0.5036236051995858\n","epoch 5\tloss : 0.693371057510376\tAcc: 0.5036236051995858\n","epoch 6\tloss : 0.6933466196060181\tAcc: 0.5036236051995858\n","epoch 7\tloss : 0.6933227181434631\tAcc: 0.5036236051995858\n","epoch 8\tloss : 0.6932989358901978\tAcc: 0.5036236051995858\n","epoch 9\tloss : 0.6932752132415771\tAcc: 0.5036236051995858\n","epoch 10\tloss : 0.6932516098022461\tAcc: 0.5036236051995858\n","epoch 11\tloss : 0.6932278871536255\tAcc: 0.5036236051995858\n","epoch 12\tloss : 0.6932045817375183\tAcc: 0.5036236051995858\n","epoch 13\tloss : 0.6931816339492798\tAcc: 0.5036236051995858\n","epoch 14\tloss : 0.6931588053703308\tAcc: 0.5036236051995858\n","epoch 15\tloss : 0.6931361556053162\tAcc: 0.5036236051995858\n","epoch 16\tloss : 0.6931135058403015\tAcc: 0.5036236051995858\n","epoch 17\tloss : 0.6930915117263794\tAcc: 0.5036236051995858\n","epoch 18\tloss : 0.6930693984031677\tAcc: 0.5036236051995858\n","epoch 19\tloss : 0.6930475234985352\tAcc: 0.5036236051995858\n","epoch 20\tloss : 0.6930252313613892\tAcc: 0.5036236051995858\n","epoch 21\tloss : 0.6930031180381775\tAcc: 0.5036236051995858\n","epoch 22\tloss : 0.6929811239242554\tAcc: 0.5036236051995858\n","epoch 23\tloss : 0.6929593086242676\tAcc: 0.5036236051995858\n","epoch 24\tloss : 0.6929373741149902\tAcc: 0.570804095249051\n","epoch 25\tloss : 0.6929156184196472\tAcc: 0.570804095249051\n","epoch 26\tloss : 0.6928938031196594\tAcc: 0.570804095249051\n","epoch 27\tloss : 0.6928720474243164\tAcc: 0.570804095249051\n","epoch 28\tloss : 0.6928502917289734\tAcc: 0.570804095249051\n","epoch 29\tloss : 0.6928285956382751\tAcc: 0.570804095249051\n","epoch 30\tloss : 0.6928070783615112\tAcc: 0.570804095249051\n","epoch 31\tloss : 0.6927855610847473\tAcc: 0.570804095249051\n","epoch 32\tloss : 0.6927639245986938\tAcc: 0.570804095249051\n","epoch 33\tloss : 0.6927423477172852\tAcc: 0.570804095249051\n","epoch 34\tloss : 0.6927207112312317\tAcc: 0.571609340848959\n","epoch 35\tloss : 0.6926990747451782\tAcc: 0.571609340848959\n","epoch 36\tloss : 0.6926777362823486\tAcc: 0.571609340848959\n","epoch 37\tloss : 0.6926565170288086\tAcc: 0.571609340848959\n","epoch 38\tloss : 0.6926353573799133\tAcc: 0.571609340848959\n","epoch 39\tloss : 0.6926143765449524\tAcc: 0.571609340848959\n","epoch 40\tloss : 0.6925936341285706\tAcc: 0.571609340848959\n","epoch 41\tloss : 0.6925728917121887\tAcc: 0.571609340848959\n","epoch 42\tloss : 0.6925522685050964\tAcc: 0.571609340848959\n","epoch 43\tloss : 0.6925316452980042\tAcc: 0.571609340848959\n","epoch 44\tloss : 0.6925113201141357\tAcc: 0.571609340848959\n","epoch 45\tloss : 0.6924907565116882\tAcc: 0.571609340848959\n","epoch 46\tloss : 0.6924704313278198\tAcc: 0.5748303232485908\n","epoch 47\tloss : 0.6924498081207275\tAcc: 0.5748303232485908\n","epoch 48\tloss : 0.6924293041229248\tAcc: 0.5748303232485908\n","epoch 49\tloss : 0.6924089193344116\tAcc: 0.5748303232485908\n","epoch 50\tloss : 0.6923885941505432\tAcc: 0.5748303232485908\n","epoch 51\tloss : 0.6923683881759644\tAcc: 0.5748303232485908\n","epoch 52\tloss : 0.6923482418060303\tAcc: 0.5748303232485908\n","epoch 53\tloss : 0.6923279762268066\tAcc: 0.5748303232485908\n","epoch 54\tloss : 0.6923075914382935\tAcc: 0.5748303232485908\n","epoch 55\tloss : 0.692287266254425\tAcc: 0.5748303232485908\n","epoch 56\tloss : 0.6922667622566223\tAcc: 0.5748303232485908\n","epoch 57\tloss : 0.6922464966773987\tAcc: 0.5748303232485908\n","epoch 58\tloss : 0.6922261714935303\tAcc: 0.5748303232485908\n","epoch 59\tloss : 0.6922062635421753\tAcc: 0.5748303232485908\n","epoch 60\tloss : 0.692186713218689\tAcc: 0.5748303232485908\n","epoch 61\tloss : 0.6921667456626892\tAcc: 0.5748303232485908\n","epoch 62\tloss : 0.6921467781066895\tAcc: 0.5748303232485908\n","epoch 63\tloss : 0.6921268701553345\tAcc: 0.5748303232485908\n","epoch 64\tloss : 0.6921070218086243\tAcc: 0.5748303232485908\n","epoch 65\tloss : 0.6920871138572693\tAcc: 0.5748303232485908\n","epoch 66\tloss : 0.6920669674873352\tAcc: 0.5748303232485908\n","epoch 67\tloss : 0.6920467019081116\tAcc: 0.5748303232485908\n","epoch 68\tloss : 0.6920264363288879\tAcc: 0.5748303232485908\n","epoch 69\tloss : 0.69200599193573\tAcc: 0.5748303232485908\n","epoch 70\tloss : 0.6919853687286377\tAcc: 0.5748303232485908\n","epoch 71\tloss : 0.6919644474983215\tAcc: 0.5748303232485908\n","epoch 72\tloss : 0.691943347454071\tAcc: 0.5748303232485908\n","epoch 73\tloss : 0.6919221878051758\tAcc: 0.5748303232485908\n","epoch 74\tloss : 0.6919004917144775\tAcc: 0.5748303232485908\n","epoch 75\tloss : 0.6918789744377136\tAcc: 0.5748303232485908\n","epoch 76\tloss : 0.6918573379516602\tAcc: 0.5748303232485908\n","epoch 77\tloss : 0.6918356418609619\tAcc: 0.5748303232485908\n","epoch 78\tloss : 0.6918138265609741\tAcc: 0.5748303232485908\n","epoch 79\tloss : 0.6917919516563416\tAcc: 0.5748303232485908\n","epoch 80\tloss : 0.6917697787284851\tAcc: 0.5748303232485908\n","epoch 81\tloss : 0.6917475461959839\tAcc: 0.5748303232485908\n","epoch 82\tloss : 0.6917252540588379\tAcc: 0.5748303232485908\n","epoch 83\tloss : 0.6917029619216919\tAcc: 0.5748303232485908\n","epoch 84\tloss : 0.6916805505752563\tAcc: 0.5748303232485908\n","epoch 85\tloss : 0.6916579008102417\tAcc: 0.5748303232485908\n","epoch 86\tloss : 0.6916353702545166\tAcc: 0.5748303232485908\n","epoch 87\tloss : 0.6916128993034363\tAcc: 0.5748303232485908\n","epoch 88\tloss : 0.6915906071662903\tAcc: 0.5748303232485908\n","epoch 89\tloss : 0.6915686726570129\tAcc: 0.5748303232485908\n","epoch 90\tloss : 0.691546618938446\tAcc: 0.5748303232485908\n","epoch 91\tloss : 0.6915245056152344\tAcc: 0.5748303232485908\n","epoch 92\tloss : 0.6915023922920227\tAcc: 0.5748303232485908\n","epoch 93\tloss : 0.6914800405502319\tAcc: 0.5748303232485908\n","epoch 94\tloss : 0.6914575695991516\tAcc: 0.5748303232485908\n","epoch 95\tloss : 0.691434383392334\tAcc: 0.5748303232485908\n","epoch 96\tloss : 0.6914111375808716\tAcc: 0.5748303232485908\n","epoch 97\tloss : 0.6913878917694092\tAcc: 0.5748303232485908\n","epoch 98\tloss : 0.6913643479347229\tAcc: 0.5748303232485908\n","epoch 99\tloss : 0.6913406848907471\tAcc: 0.5748303232485908\n","epoch 100\tloss : 0.691317081451416\tAcc: 0.5748303232485908\n","epoch 101\tloss : 0.6912932991981506\tAcc: 0.5748303232485908\n","epoch 102\tloss : 0.6912694573402405\tAcc: 0.5748303232485908\n","epoch 103\tloss : 0.6912457346916199\tAcc: 0.5748303232485908\n","epoch 104\tloss : 0.6912215948104858\tAcc: 0.5748303232485908\n","epoch 105\tloss : 0.6911972761154175\tAcc: 0.5748303232485908\n","epoch 106\tloss : 0.6911728978157043\tAcc: 0.5748303232485908\n","epoch 107\tloss : 0.6911482810974121\tAcc: 0.5748303232485908\n","epoch 108\tloss : 0.6911234259605408\tAcc: 0.5748303232485908\n","epoch 109\tloss : 0.691098690032959\tAcc: 0.5748303232485908\n","epoch 110\tloss : 0.6910737156867981\tAcc: 0.5748303232485908\n","epoch 111\tloss : 0.6910485029220581\tAcc: 0.5748303232485908\n","epoch 112\tloss : 0.6910229325294495\tAcc: 0.5748303232485908\n","epoch 113\tloss : 0.6909977197647095\tAcc: 0.5748303232485908\n","epoch 114\tloss : 0.6909719705581665\tAcc: 0.5748303232485908\n","epoch 115\tloss : 0.6909462213516235\tAcc: 0.5748303232485908\n","epoch 116\tloss : 0.6909204125404358\tAcc: 0.5748303232485908\n","epoch 117\tloss : 0.6908943057060242\tAcc: 0.5748303232485908\n","epoch 118\tloss : 0.6908677816390991\tAcc: 0.5748303232485908\n","epoch 119\tloss : 0.6908411979675293\tAcc: 0.5748303232485908\n","epoch 120\tloss : 0.6908146142959595\tAcc: 0.5748303232485908\n","epoch 121\tloss : 0.6907879114151001\tAcc: 0.5748303232485908\n","epoch 122\tloss : 0.6907608509063721\tAcc: 0.5748303232485908\n","epoch 123\tloss : 0.6907337307929993\tAcc: 0.5748303232485908\n","epoch 124\tloss : 0.6907066106796265\tAcc: 0.5748303232485908\n","epoch 125\tloss : 0.690679132938385\tAcc: 0.5748303232485908\n","epoch 126\tloss : 0.6906512379646301\tAcc: 0.5748303232485908\n","epoch 127\tloss : 0.69062340259552\tAcc: 0.5748303232485908\n","epoch 128\tloss : 0.690595269203186\tAcc: 0.5748303232485908\n","epoch 129\tloss : 0.690566897392273\tAcc: 0.5748303232485908\n","epoch 130\tloss : 0.6905385851860046\tAcc: 0.5748303232485908\n","epoch 131\tloss : 0.6905102133750916\tAcc: 0.5748303232485908\n","epoch 132\tloss : 0.6904816627502441\tAcc: 0.5748303232485908\n","epoch 133\tloss : 0.6904524564743042\tAcc: 0.5748303232485908\n","epoch 134\tloss : 0.6904226541519165\tAcc: 0.5748303232485908\n","epoch 135\tloss : 0.690392792224884\tAcc: 0.5748303232485908\n","epoch 136\tloss : 0.690362811088562\tAcc: 0.5748303232485908\n","epoch 137\tloss : 0.6903324723243713\tAcc: 0.5748303232485908\n","epoch 138\tloss : 0.6903017163276672\tAcc: 0.5748303232485908\n","epoch 139\tloss : 0.6902707815170288\tAcc: 0.5748303232485908\n","epoch 140\tloss : 0.690239667892456\tAcc: 0.5748303232485908\n","epoch 141\tloss : 0.6902081966400146\tAcc: 0.5748303232485908\n","epoch 142\tloss : 0.6901766657829285\tAcc: 0.5748303232485908\n","epoch 143\tloss : 0.6901448965072632\tAcc: 0.5748303232485908\n","epoch 144\tloss : 0.6901131272315979\tAcc: 0.5748303232485908\n","epoch 145\tloss : 0.6900811791419983\tAcc: 0.5748303232485908\n","epoch 146\tloss : 0.6900489330291748\tAcc: 0.5748303232485908\n","epoch 147\tloss : 0.6900166273117065\tAcc: 0.5748303232485908\n","epoch 148\tloss : 0.6899843811988831\tAcc: 0.5748303232485908\n","epoch 149\tloss : 0.6899518966674805\tAcc: 0.5748303232485908\n","epoch 150\tloss : 0.689919114112854\tAcc: 0.5748303232485908\n","epoch 151\tloss : 0.689886212348938\tAcc: 0.5748303232485908\n","epoch 152\tloss : 0.6898528933525085\tAcc: 0.5748303232485908\n","epoch 153\tloss : 0.6898193359375\tAcc: 0.5748303232485908\n","epoch 154\tloss : 0.689785361289978\tAcc: 0.5748303232485908\n","epoch 155\tloss : 0.6897510290145874\tAcc: 0.5748303232485908\n","epoch 156\tloss : 0.6897163391113281\tAcc: 0.5748303232485908\n","epoch 157\tloss : 0.6896815896034241\tAcc: 0.5748303232485908\n","epoch 158\tloss : 0.6896466016769409\tAcc: 0.5748303232485908\n","epoch 159\tloss : 0.689611554145813\tAcc: 0.5748303232485908\n","epoch 160\tloss : 0.6895759701728821\tAcc: 0.5748303232485908\n","epoch 161\tloss : 0.6895402669906616\tAcc: 0.5902450247325435\n","epoch 162\tloss : 0.6895044445991516\tAcc: 0.5902450247325435\n","epoch 163\tloss : 0.6894682049751282\tAcc: 0.5902450247325435\n","epoch 164\tloss : 0.6894317269325256\tAcc: 0.5902450247325435\n","epoch 165\tloss : 0.6893945932388306\tAcc: 0.5902450247325435\n","epoch 166\tloss : 0.6893571019172668\tAcc: 0.5902450247325435\n","epoch 167\tloss : 0.689319372177124\tAcc: 0.5902450247325435\n","epoch 168\tloss : 0.6892814636230469\tAcc: 0.5902450247325435\n","epoch 169\tloss : 0.6892433166503906\tAcc: 0.5902450247325435\n","epoch 170\tloss : 0.6892050504684448\tAcc: 0.5902450247325435\n","epoch 171\tloss : 0.6891666054725647\tAcc: 0.5902450247325435\n","epoch 172\tloss : 0.6891279220581055\tAcc: 0.5902450247325435\n","epoch 173\tloss : 0.6890892386436462\tAcc: 0.5902450247325435\n","epoch 174\tloss : 0.6890504956245422\tAcc: 0.5902450247325435\n","epoch 175\tloss : 0.6890113949775696\tAcc: 0.5902450247325435\n","epoch 176\tloss : 0.688971996307373\tAcc: 0.5902450247325435\n","epoch 177\tloss : 0.6889322996139526\tAcc: 0.5902450247325435\n","epoch 178\tloss : 0.6888922452926636\tAcc: 0.5902450247325435\n","epoch 179\tloss : 0.6888515949249268\tAcc: 0.5902450247325435\n","epoch 180\tloss : 0.6888108253479004\tAcc: 0.5902450247325435\n","epoch 181\tloss : 0.6887697577476501\tAcc: 0.5902450247325435\n","epoch 182\tloss : 0.6887285113334656\tAcc: 0.5902450247325435\n","epoch 183\tloss : 0.688686728477478\tAcc: 0.5902450247325435\n","epoch 184\tloss : 0.6886449456214905\tAcc: 0.5902450247325435\n","epoch 185\tloss : 0.6886029243469238\tAcc: 0.5902450247325435\n","epoch 186\tloss : 0.6885604858398438\tAcc: 0.5902450247325435\n","epoch 187\tloss : 0.6885178089141846\tAcc: 0.5902450247325435\n","epoch 188\tloss : 0.6884745955467224\tAcc: 0.5902450247325435\n","epoch 189\tloss : 0.6884310245513916\tAcc: 0.5902450247325435\n","epoch 190\tloss : 0.6883870363235474\tAcc: 0.5902450247325435\n","epoch 191\tloss : 0.6883429288864136\tAcc: 0.5902450247325435\n","epoch 192\tloss : 0.6882979869842529\tAcc: 0.5902450247325435\n","epoch 193\tloss : 0.6882526874542236\tAcc: 0.5909352352467503\n","epoch 194\tloss : 0.6882070302963257\tAcc: 0.5909352352467503\n","epoch 195\tloss : 0.6881610751152039\tAcc: 0.5909352352467503\n","epoch 196\tloss : 0.6881147623062134\tAcc: 0.5909352352467503\n","epoch 197\tloss : 0.6880683302879333\tAcc: 0.5909352352467503\n","epoch 198\tloss : 0.6880214810371399\tAcc: 0.5909352352467503\n","epoch 199\tloss : 0.6879742741584778\tAcc: 0.5909352352467503\n","epoch 200\tloss : 0.687926709651947\tAcc: 0.5909352352467503\n","epoch 201\tloss : 0.6878786087036133\tAcc: 0.5909352352467503\n","epoch 202\tloss : 0.6878299713134766\tAcc: 0.5909352352467503\n","epoch 203\tloss : 0.6877812147140503\tAcc: 0.5909352352467503\n","epoch 204\tloss : 0.6877320408821106\tAcc: 0.5909352352467503\n","epoch 205\tloss : 0.6876825094223022\tAcc: 0.5909352352467503\n","epoch 206\tloss : 0.68763267993927\tAcc: 0.5909352352467503\n","epoch 207\tloss : 0.68758225440979\tAcc: 0.5909352352467503\n","epoch 208\tloss : 0.6875316500663757\tAcc: 0.5909352352467503\n","epoch 209\tloss : 0.6874808073043823\tAcc: 0.5909352352467503\n","epoch 210\tloss : 0.6874293684959412\tAcc: 0.5909352352467503\n","epoch 211\tloss : 0.6873777508735657\tAcc: 0.5909352352467503\n","epoch 212\tloss : 0.6873255968093872\tAcc: 0.5909352352467503\n","epoch 213\tloss : 0.6872732639312744\tAcc: 0.5909352352467503\n","epoch 214\tloss : 0.6872204542160034\tAcc: 0.5909352352467503\n","epoch 215\tloss : 0.6871674060821533\tAcc: 0.5909352352467503\n","epoch 216\tloss : 0.6871136426925659\tAcc: 0.5909352352467503\n","epoch 217\tloss : 0.6870595812797546\tAcc: 0.5909352352467503\n","epoch 218\tloss : 0.6870047450065613\tAcc: 0.5909352352467503\n","epoch 219\tloss : 0.686949610710144\tAcc: 0.5909352352467503\n","epoch 220\tloss : 0.6868939995765686\tAcc: 0.5909352352467503\n","epoch 221\tloss : 0.6868382096290588\tAcc: 0.5909352352467503\n","epoch 222\tloss : 0.6867818832397461\tAcc: 0.5909352352467503\n","epoch 223\tloss : 0.6867250204086304\tAcc: 0.5909352352467503\n","epoch 224\tloss : 0.6866678595542908\tAcc: 0.5909352352467503\n","epoch 225\tloss : 0.6866099238395691\tAcc: 0.5909352352467503\n","epoch 226\tloss : 0.6865515112876892\tAcc: 0.5909352352467503\n","epoch 227\tloss : 0.6864926815032959\tAcc: 0.5909352352467503\n","epoch 228\tloss : 0.6864334940910339\tAcc: 0.5909352352467503\n","epoch 229\tloss : 0.6863739490509033\tAcc: 0.5909352352467503\n","epoch 230\tloss : 0.6863139271736145\tAcc: 0.5909352352467503\n","epoch 231\tloss : 0.6862534284591675\tAcc: 0.5909352352467503\n","epoch 232\tloss : 0.6861927509307861\tAcc: 0.5909352352467503\n","epoch 233\tloss : 0.6861318945884705\tAcc: 0.5909352352467503\n","epoch 234\tloss : 0.6860703825950623\tAcc: 0.5909352352467503\n","epoch 235\tloss : 0.6860084533691406\tAcc: 0.5909352352467503\n","epoch 236\tloss : 0.6859458684921265\tAcc: 0.5909352352467503\n","epoch 237\tloss : 0.6858828067779541\tAcc: 0.5909352352467503\n","epoch 238\tloss : 0.6858194470405579\tAcc: 0.5909352352467503\n","epoch 239\tloss : 0.6857554912567139\tAcc: 0.5909352352467503\n","epoch 240\tloss : 0.6856910586357117\tAcc: 0.5909352352467503\n","epoch 241\tloss : 0.6856261491775513\tAcc: 0.5909352352467503\n","epoch 242\tloss : 0.685560405254364\tAcc: 0.5909352352467503\n","epoch 243\tloss : 0.6854941844940186\tAcc: 0.5909352352467503\n","epoch 244\tloss : 0.6854274272918701\tAcc: 0.5909352352467503\n","epoch 245\tloss : 0.6853606104850769\tAcc: 0.5909352352467503\n","epoch 246\tloss : 0.6852931380271912\tAcc: 0.5909352352467503\n","epoch 247\tloss : 0.6852250695228577\tAcc: 0.5909352352467503\n","epoch 248\tloss : 0.685156524181366\tAcc: 0.5909352352467503\n","epoch 249\tloss : 0.6850875020027161\tAcc: 0.5909352352467503\n","epoch 250\tloss : 0.685018002986908\tAcc: 0.5909352352467503\n","epoch 251\tloss : 0.6849479079246521\tAcc: 0.5909352352467503\n","epoch 252\tloss : 0.6848772764205933\tAcc: 0.5909352352467503\n","epoch 253\tloss : 0.6848064661026001\tAcc: 0.5909352352467503\n","epoch 254\tloss : 0.6847347617149353\tAcc: 0.5909352352467503\n","epoch 255\tloss : 0.6846626400947571\tAcc: 0.5909352352467503\n","epoch 256\tloss : 0.6845895051956177\tAcc: 0.5909352352467503\n","epoch 257\tloss : 0.6845160126686096\tAcc: 0.5909352352467503\n","epoch 258\tloss : 0.6844419240951538\tAcc: 0.5909352352467503\n","epoch 259\tloss : 0.6843676567077637\tAcc: 0.5909352352467503\n","epoch 260\tloss : 0.684293270111084\tAcc: 0.5909352352467503\n","epoch 261\tloss : 0.6842185854911804\tAcc: 0.5909352352467503\n","epoch 262\tloss : 0.6841429471969604\tAcc: 0.5909352352467503\n","epoch 263\tloss : 0.6840665340423584\tAcc: 0.5909352352467503\n","epoch 264\tloss : 0.683989405632019\tAcc: 0.5909352352467503\n","epoch 265\tloss : 0.683911919593811\tAcc: 0.5909352352467503\n","epoch 266\tloss : 0.6838338375091553\tAcc: 0.5909352352467503\n","epoch 267\tloss : 0.6837551593780518\tAcc: 0.5909352352467503\n","epoch 268\tloss : 0.6836763620376587\tAcc: 0.5909352352467503\n","epoch 269\tloss : 0.6835964918136597\tAcc: 0.5909352352467503\n","epoch 270\tloss : 0.6835166811943054\tAcc: 0.5909352352467503\n","epoch 271\tloss : 0.6834360957145691\tAcc: 0.5909352352467503\n","epoch 272\tloss : 0.6833551526069641\tAcc: 0.5909352352467503\n","epoch 273\tloss : 0.6832737326622009\tAcc: 0.5909352352467503\n","epoch 274\tloss : 0.6831920146942139\tAcc: 0.5909352352467503\n","epoch 275\tloss : 0.6831097602844238\tAcc: 0.5909352352467503\n","epoch 276\tloss : 0.683027446269989\tAcc: 0.5909352352467503\n","epoch 277\tloss : 0.6829444169998169\tAcc: 0.5909352352467503\n","epoch 278\tloss : 0.6828606724739075\tAcc: 0.5909352352467503\n","epoch 279\tloss : 0.6827763319015503\tAcc: 0.5909352352467503\n","epoch 280\tloss : 0.6826919913291931\tAcc: 0.5909352352467503\n","epoch 281\tloss : 0.6826068758964539\tAcc: 0.5909352352467503\n","epoch 282\tloss : 0.6825218200683594\tAcc: 0.5909352352467503\n","epoch 283\tloss : 0.682435929775238\tAcc: 0.5909352352467503\n","epoch 284\tloss : 0.6823500394821167\tAcc: 0.5909352352467503\n","epoch 285\tloss : 0.6822633147239685\tAcc: 0.5909352352467503\n","epoch 286\tloss : 0.6821760535240173\tAcc: 0.5909352352467503\n","epoch 287\tloss : 0.6820884943008423\tAcc: 0.5909352352467503\n","epoch 288\tloss : 0.6820003986358643\tAcc: 0.5909352352467503\n","epoch 289\tloss : 0.6819115281105042\tAcc: 0.5909352352467503\n","epoch 290\tloss : 0.6818225979804993\tAcc: 0.5909352352467503\n","epoch 291\tloss : 0.681732714176178\tAcc: 0.5909352352467503\n","epoch 292\tloss : 0.6816427707672119\tAcc: 0.5909352352467503\n","epoch 293\tloss : 0.6815521121025085\tAcc: 0.5909352352467503\n","epoch 294\tloss : 0.6814610362052917\tAcc: 0.5909352352467503\n","epoch 295\tloss : 0.6813697814941406\tAcc: 0.5909352352467503\n","epoch 296\tloss : 0.6812776923179626\tAcc: 0.5909352352467503\n","epoch 297\tloss : 0.6811853051185608\tAcc: 0.5909352352467503\n","epoch 298\tloss : 0.6810919642448425\tAcc: 0.5909352352467503\n","epoch 299\tloss : 0.6809982061386108\tAcc: 0.5909352352467503\n","epoch 300\tloss : 0.680903971195221\tAcc: 0.5909352352467503\n","epoch 301\tloss : 0.6808092594146729\tAcc: 0.5909352352467503\n","epoch 302\tloss : 0.680713951587677\tAcc: 0.5909352352467503\n","epoch 303\tloss : 0.6806187629699707\tAcc: 0.5909352352467503\n","epoch 304\tloss : 0.680523157119751\tAcc: 0.5909352352467503\n","epoch 305\tloss : 0.6804278492927551\tAcc: 0.5909352352467503\n","epoch 306\tloss : 0.6803320646286011\tAcc: 0.5909352352467503\n","epoch 307\tloss : 0.6802350878715515\tAcc: 0.5909352352467503\n","epoch 308\tloss : 0.6801377534866333\tAcc: 0.5909352352467503\n","epoch 309\tloss : 0.6800398230552673\tAcc: 0.5909352352467503\n","epoch 310\tloss : 0.6799417734146118\tAcc: 0.5909352352467503\n","epoch 311\tloss : 0.6798439025878906\tAcc: 0.5909352352467503\n","epoch 312\tloss : 0.6797449588775635\tAcc: 0.5909352352467503\n","epoch 313\tloss : 0.6796459555625916\tAcc: 0.5909352352467503\n","epoch 314\tloss : 0.679546594619751\tAcc: 0.5909352352467503\n","epoch 315\tloss : 0.6794468760490417\tAcc: 0.5909352352467503\n","epoch 316\tloss : 0.6793467402458191\tAcc: 0.5909352352467503\n","epoch 317\tloss : 0.6792455911636353\tAcc: 0.5909352352467503\n","epoch 318\tloss : 0.6791445016860962\tAcc: 0.5909352352467503\n","epoch 319\tloss : 0.6790428161621094\tAcc: 0.5909352352467503\n","epoch 320\tloss : 0.6789412498474121\tAcc: 0.5909352352467503\n","epoch 321\tloss : 0.6788390278816223\tAcc: 0.5909352352467503\n","epoch 322\tloss : 0.6787369251251221\tAcc: 0.5909352352467503\n","epoch 323\tloss : 0.6786342263221741\tAcc: 0.5909352352467503\n","epoch 324\tloss : 0.6785311698913574\tAcc: 0.5909352352467503\n","epoch 325\tloss : 0.6784279942512512\tAcc: 0.5909352352467503\n","epoch 326\tloss : 0.6783240437507629\tAcc: 0.5909352352467503\n","epoch 327\tloss : 0.6782198548316956\tAcc: 0.5909352352467503\n","epoch 328\tloss : 0.6781152486801147\tAcc: 0.5909352352467503\n","epoch 329\tloss : 0.6780104041099548\tAcc: 0.5909352352467503\n","epoch 330\tloss : 0.6779052019119263\tAcc: 0.5909352352467503\n","epoch 331\tloss : 0.6777992844581604\tAcc: 0.5909352352467503\n","epoch 332\tloss : 0.6776930689811707\tAcc: 0.5909352352467503\n","epoch 333\tloss : 0.677586555480957\tAcc: 0.5909352352467503\n","epoch 334\tloss : 0.6774799823760986\tAcc: 0.5909352352467503\n","epoch 335\tloss : 0.6773728728294373\tAcc: 0.5909352352467503\n","epoch 336\tloss : 0.6772653460502625\tAcc: 0.5909352352467503\n","epoch 337\tloss : 0.6771577000617981\tAcc: 0.5909352352467503\n","epoch 338\tloss : 0.6770498752593994\tAcc: 0.5909352352467503\n","epoch 339\tloss : 0.6769417524337769\tAcc: 0.5909352352467503\n","epoch 340\tloss : 0.6768338680267334\tAcc: 0.5909352352467503\n","epoch 341\tloss : 0.6767255067825317\tAcc: 0.5909352352467503\n","epoch 342\tloss : 0.6766173839569092\tAcc: 0.5909352352467503\n","epoch 343\tloss : 0.6765091419219971\tAcc: 0.5909352352467503\n","epoch 344\tloss : 0.6764003038406372\tAcc: 0.5909352352467503\n","epoch 345\tloss : 0.6762923002243042\tAcc: 0.5909352352467503\n","epoch 346\tloss : 0.6761836409568787\tAcc: 0.5909352352467503\n","epoch 347\tloss : 0.6760746240615845\tAcc: 0.5909352352467503\n","epoch 348\tloss : 0.6759658455848694\tAcc: 0.5909352352467503\n","epoch 349\tloss : 0.6758565902709961\tAcc: 0.5909352352467503\n","epoch 350\tloss : 0.6757476925849915\tAcc: 0.5909352352467503\n","epoch 351\tloss : 0.6756389737129211\tAcc: 0.5909352352467503\n","epoch 352\tloss : 0.6755298376083374\tAcc: 0.5909352352467503\n","epoch 353\tloss : 0.6754205226898193\tAcc: 0.5909352352467503\n","epoch 354\tloss : 0.6753108501434326\tAcc: 0.5909352352467503\n","epoch 355\tloss : 0.675200879573822\tAcc: 0.5909352352467503\n","epoch 356\tloss : 0.675091564655304\tAcc: 0.5909352352467503\n","epoch 357\tloss : 0.6749815344810486\tAcc: 0.5909352352467503\n","epoch 358\tloss : 0.6748718023300171\tAcc: 0.5909352352467503\n","epoch 359\tloss : 0.6747618317604065\tAcc: 0.5909352352467503\n","epoch 360\tloss : 0.6746522188186646\tAcc: 0.5909352352467503\n","epoch 361\tloss : 0.6745426654815674\tAcc: 0.5909352352467503\n","epoch 362\tloss : 0.6744328141212463\tAcc: 0.5909352352467503\n","epoch 363\tloss : 0.6743229031562805\tAcc: 0.5909352352467503\n","epoch 364\tloss : 0.6742138266563416\tAcc: 0.5909352352467503\n","epoch 365\tloss : 0.6741044521331787\tAcc: 0.5909352352467503\n","epoch 366\tloss : 0.6739950180053711\tAcc: 0.5909352352467503\n","epoch 367\tloss : 0.6738862991333008\tAcc: 0.5909352352467503\n","epoch 368\tloss : 0.6737774610519409\tAcc: 0.5909352352467503\n","epoch 369\tloss : 0.6736685633659363\tAcc: 0.5909352352467503\n","epoch 370\tloss : 0.6735602021217346\tAcc: 0.5909352352467503\n","epoch 371\tloss : 0.6734513640403748\tAcc: 0.5909352352467503\n","epoch 372\tloss : 0.6733431816101074\tAcc: 0.5909352352467503\n","epoch 373\tloss : 0.673234760761261\tAcc: 0.5909352352467503\n","epoch 374\tloss : 0.6731270551681519\tAcc: 0.5909352352467503\n","epoch 375\tloss : 0.6730190515518188\tAcc: 0.5909352352467503\n","epoch 376\tloss : 0.6729117631912231\tAcc: 0.5909352352467503\n","epoch 377\tloss : 0.6728047728538513\tAcc: 0.5909352352467503\n","epoch 378\tloss : 0.6726974844932556\tAcc: 0.5909352352467503\n","epoch 379\tloss : 0.6725903153419495\tAcc: 0.5909352352467503\n","epoch 380\tloss : 0.6724842190742493\tAcc: 0.5909352352467503\n","epoch 381\tloss : 0.6723781824111938\tAcc: 0.5909352352467503\n","epoch 382\tloss : 0.6722726821899414\tAcc: 0.5909352352467503\n","epoch 383\tloss : 0.6721673607826233\tAcc: 0.5909352352467503\n","epoch 384\tloss : 0.6720622777938843\tAcc: 0.5909352352467503\n","epoch 385\tloss : 0.6719574928283691\tAcc: 0.5909352352467503\n","epoch 386\tloss : 0.6718529462814331\tAcc: 0.5909352352467503\n","epoch 387\tloss : 0.6717492938041687\tAcc: 0.5909352352467503\n","epoch 388\tloss : 0.6716464757919312\tAcc: 0.5909352352467503\n","epoch 389\tloss : 0.671542763710022\tAcc: 0.5909352352467503\n","epoch 390\tloss : 0.671440064907074\tAcc: 0.5909352352467503\n","epoch 391\tloss : 0.6713384389877319\tAcc: 0.5909352352467503\n","epoch 392\tloss : 0.6712365746498108\tAcc: 0.5909352352467503\n","epoch 393\tloss : 0.6711359024047852\tAcc: 0.5909352352467503\n","epoch 394\tloss : 0.6710345149040222\tAcc: 0.5909352352467503\n","epoch 395\tloss : 0.6709350347518921\tAcc: 0.5909352352467503\n","epoch 396\tloss : 0.670835018157959\tAcc: 0.5909352352467503\n","epoch 397\tloss : 0.6707357168197632\tAcc: 0.5909352352467503\n","epoch 398\tloss : 0.6706371307373047\tAcc: 0.5909352352467503\n","epoch 399\tloss : 0.6705392599105835\tAcc: 0.5909352352467503\n","epoch 400\tloss : 0.6704419255256653\tAcc: 0.5909352352467503\n","epoch 401\tloss : 0.6703445315361023\tAcc: 0.5909352352467503\n","epoch 402\tloss : 0.6702480912208557\tAcc: 0.5909352352467503\n","epoch 403\tloss : 0.6701520681381226\tAcc: 0.5909352352467503\n","epoch 404\tloss : 0.6700559854507446\tAcc: 0.5909352352467503\n","epoch 405\tloss : 0.6699614524841309\tAcc: 0.5909352352467503\n","epoch 406\tloss : 0.6698667407035828\tAcc: 0.5909352352467503\n","epoch 407\tloss : 0.6697725057601929\tAcc: 0.5909352352467503\n","epoch 408\tloss : 0.6696795225143433\tAcc: 0.5909352352467503\n","epoch 409\tloss : 0.6695868968963623\tAcc: 0.5909352352467503\n","epoch 410\tloss : 0.6694952845573425\tAcc: 0.5909352352467503\n","epoch 411\tloss : 0.6694035530090332\tAcc: 0.5909352352467503\n","epoch 412\tloss : 0.6693127751350403\tAcc: 0.5909352352467503\n","epoch 413\tloss : 0.6692224740982056\tAcc: 0.5909352352467503\n","epoch 414\tloss : 0.6691334247589111\tAcc: 0.5909352352467503\n","epoch 415\tloss : 0.6690449714660645\tAcc: 0.5909352352467503\n","epoch 416\tloss : 0.6689562797546387\tAcc: 0.5909352352467503\n","epoch 417\tloss : 0.6688690781593323\tAcc: 0.5909352352467503\n","epoch 418\tloss : 0.6687824726104736\tAcc: 0.5909352352467503\n","epoch 419\tloss : 0.668695867061615\tAcc: 0.5909352352467503\n","epoch 420\tloss : 0.6686109304428101\tAcc: 0.5909352352467503\n","epoch 421\tloss : 0.6685263514518738\tAcc: 0.5909352352467503\n","epoch 422\tloss : 0.6684420704841614\tAcc: 0.5909352352467503\n","epoch 423\tloss : 0.6683589816093445\tAcc: 0.5909352352467503\n","epoch 424\tloss : 0.6682772040367126\tAcc: 0.5909352352467503\n","epoch 425\tloss : 0.6681948900222778\tAcc: 0.5909352352467503\n","epoch 426\tloss : 0.6681143641471863\tAcc: 0.5909352352467503\n","epoch 427\tloss : 0.6680334806442261\tAcc: 0.5909352352467503\n","epoch 428\tloss : 0.667953610420227\tAcc: 0.5909352352467503\n","epoch 429\tloss : 0.6678747534751892\tAcc: 0.5909352352467503\n","epoch 430\tloss : 0.6677969694137573\tAcc: 0.5909352352467503\n","epoch 431\tloss : 0.6677190065383911\tAcc: 0.5909352352467503\n","epoch 432\tloss : 0.6676422953605652\tAcc: 0.5909352352467503\n","epoch 433\tloss : 0.6675660014152527\tAcc: 0.5909352352467503\n","epoch 434\tloss : 0.6674908399581909\tAcc: 0.5909352352467503\n","epoch 435\tloss : 0.6674157977104187\tAcc: 0.5909352352467503\n","epoch 436\tloss : 0.6673420071601868\tAcc: 0.5909352352467503\n","epoch 437\tloss : 0.6672683954238892\tAcc: 0.5909352352467503\n","epoch 438\tloss : 0.6671956777572632\tAcc: 0.5909352352467503\n","epoch 439\tloss : 0.6671232581138611\tAcc: 0.5909352352467503\n","epoch 440\tloss : 0.6670516133308411\tAcc: 0.5909352352467503\n","epoch 441\tloss : 0.6669809818267822\tAcc: 0.5909352352467503\n","epoch 442\tloss : 0.6669114828109741\tAcc: 0.5909352352467503\n","epoch 443\tloss : 0.6668422222137451\tAcc: 0.5909352352467503\n","epoch 444\tloss : 0.6667740345001221\tAcc: 0.5909352352467503\n","epoch 445\tloss : 0.6667066812515259\tAcc: 0.5909352352467503\n","epoch 446\tloss : 0.6666398048400879\tAcc: 0.5909352352467503\n","epoch 447\tloss : 0.6665732860565186\tAcc: 0.5909352352467503\n","epoch 448\tloss : 0.6665075421333313\tAcc: 0.5909352352467503\n","epoch 449\tloss : 0.6664430499076843\tAcc: 0.5909352352467503\n","epoch 450\tloss : 0.6663788557052612\tAcc: 0.5909352352467503\n","epoch 451\tloss : 0.6663150191307068\tAcc: 0.5909352352467503\n","epoch 452\tloss : 0.6662524938583374\tAcc: 0.5909352352467503\n","epoch 453\tloss : 0.6661903262138367\tAcc: 0.5909352352467503\n","epoch 454\tloss : 0.6661288142204285\tAcc: 0.5909352352467503\n","epoch 455\tloss : 0.6660681366920471\tAcc: 0.5909352352467503\n","epoch 456\tloss : 0.6660078763961792\tAcc: 0.5909352352467503\n","epoch 457\tloss : 0.6659483909606934\tAcc: 0.5909352352467503\n","epoch 458\tloss : 0.6658895015716553\tAcc: 0.5909352352467503\n","epoch 459\tloss : 0.6658312082290649\tAcc: 0.5909352352467503\n","epoch 460\tloss : 0.665773868560791\tAcc: 0.5909352352467503\n","epoch 461\tloss : 0.6657171249389648\tAcc: 0.5909352352467503\n","epoch 462\tloss : 0.6656612753868103\tAcc: 0.5909352352467503\n","epoch 463\tloss : 0.6656062602996826\tAcc: 0.5909352352467503\n","epoch 464\tloss : 0.665552020072937\tAcc: 0.5909352352467503\n","epoch 465\tloss : 0.6654981374740601\tAcc: 0.5909352352467503\n","epoch 466\tloss : 0.6654453873634338\tAcc: 0.5909352352467503\n","epoch 467\tloss : 0.6653929948806763\tAcc: 0.5909352352467503\n","epoch 468\tloss : 0.6653414964675903\tAcc: 0.5909352352467503\n","epoch 469\tloss : 0.6652897596359253\tAcc: 0.5909352352467503\n","epoch 470\tloss : 0.6652398705482483\tAcc: 0.5909352352467503\n","epoch 471\tloss : 0.6651896238327026\tAcc: 0.5909352352467503\n","epoch 472\tloss : 0.6651408076286316\tAcc: 0.5909352352467503\n","epoch 473\tloss : 0.6650925278663635\tAcc: 0.5909352352467503\n","epoch 474\tloss : 0.6650443077087402\tAcc: 0.5909352352467503\n","epoch 475\tloss : 0.6649965047836304\tAcc: 0.5909352352467503\n","epoch 476\tloss : 0.6649495959281921\tAcc: 0.5909352352467503\n","epoch 477\tloss : 0.6649037003517151\tAcc: 0.5909352352467503\n","epoch 478\tloss : 0.6648580431938171\tAcc: 0.5909352352467503\n","epoch 479\tloss : 0.6648134589195251\tAcc: 0.5909352352467503\n","epoch 480\tloss : 0.6647690534591675\tAcc: 0.5909352352467503\n","epoch 481\tloss : 0.6647258400917053\tAcc: 0.5909352352467503\n","epoch 482\tloss : 0.6646825075149536\tAcc: 0.5909352352467503\n","epoch 483\tloss : 0.6646397113800049\tAcc: 0.5909352352467503\n","epoch 484\tloss : 0.6645984053611755\tAcc: 0.5909352352467503\n","epoch 485\tloss : 0.6645564436912537\tAcc: 0.5909352352467503\n","epoch 486\tloss : 0.6645154356956482\tAcc: 0.5909352352467503\n","epoch 487\tloss : 0.6644747853279114\tAcc: 0.5909352352467503\n","epoch 488\tloss : 0.6644350290298462\tAcc: 0.5909352352467503\n","epoch 489\tloss : 0.6643953323364258\tAcc: 0.5909352352467503\n","epoch 490\tloss : 0.6643561124801636\tAcc: 0.5909352352467503\n","epoch 491\tloss : 0.6643174886703491\tAcc: 0.5909352352467503\n","epoch 492\tloss : 0.6642794013023376\tAcc: 0.5909352352467503\n","epoch 493\tloss : 0.6642417311668396\tAcc: 0.5909352352467503\n","epoch 494\tloss : 0.6642045974731445\tAcc: 0.5909352352467503\n","epoch 495\tloss : 0.6641675233840942\tAcc: 0.5909352352467503\n","epoch 496\tloss : 0.664131224155426\tAcc: 0.5909352352467503\n","epoch 497\tloss : 0.6640950441360474\tAcc: 0.5909352352467503\n","epoch 498\tloss : 0.6640605330467224\tAcc: 0.5909352352467503\n","epoch 499\tloss : 0.6640253067016602\tAcc: 0.5909352352467503\n","epoch 500\tloss : 0.6639907360076904\tAcc: 0.5909352352467503\n","Saving NN to ./model_clean.pth\n"]}],"source":["modelPath = \"./model_clean.pth\"\n","train(model,epochs=500,model_path=modelPath,log_path=\"./log_noiseSet/\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["testModel = Net(x.shape[1])\n","mapped = test(testModel,model_path=modelPath,testDataPath = \"./dataset/test_denoised.csv\")\n","df = pd.DataFrame.from_dict(mapped)\n","df[\"Transported\"] = df[\"Transported\"].astype(bool)\n","df.to_csv(\"./dataset/result.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":".Kaggle","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":2}
