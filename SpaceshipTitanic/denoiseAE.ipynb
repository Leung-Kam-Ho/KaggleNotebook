{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from tensorboardX import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.data = x\n",
    "        self.label = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(AE_Encoder,self).__init__()\n",
    "    self.fc1 = nn.Linear(23,20)\n",
    "    self.fc2 = nn.Linear(20,10)\n",
    "    self.fc3 = nn.Linear(10,5)\n",
    "    self.fc4 = nn.Linear(5,3)\n",
    "  def forward(self,x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.relu(self.fc4(x))\n",
    "    return x\n",
    "class AE_Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(AE_Decoder,self).__init__()\n",
    "    self.fc4 = nn.Linear(20,23)\n",
    "    self.fc3 = nn.Linear(10,20)\n",
    "    self.fc2 = nn.Linear(5,10)\n",
    "    self.fc1 = nn.Linear(3,5)\n",
    "  def forward(self,x):\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = torch.relu(self.fc3(x))\n",
    "    x = torch.relu(self.fc4(x))\n",
    "    x = torch.sigmoid(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "lr= 0.001\n",
    "encoder = AE_Encoder()\n",
    "decoder = AE_Decoder()\n",
    "params_to_optimize = [\n",
    "\t{'params': encoder.parameters()},\n",
    "\t{'params': decoder.parameters()}\n",
    "]\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "# device = torch.device(\"mps\")\n",
    "# encoder.to(device)\n",
    "# decoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7736, 23)\n",
      "(7736, 23)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"./dataset/train_preprocessed_clean.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "y = df[\"Transported\"]\n",
    "df = df.drop(\"Transported\", axis=1)\n",
    "x = df.values\n",
    "print(x.shape)\n",
    "noise_csv_file_path = \"./dataset/train_noise.csv\"\n",
    "ndf = pd.read_csv(noise_csv_file_path)\n",
    "ny = ndf[\"Transported\"]\n",
    "ndf = ndf.drop(\"Transported\", axis=1)\n",
    "nx = ndf.values\n",
    "print(nx.shape)\n",
    "train_data_object = CustomDataSet(x,y)\n",
    "noise_data_object = CustomDataSet(nx,ny)\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_data_object,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "noise_train_loader = torch.utils.data.DataLoader(\n",
    "        noise_data_object,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "for (data,_),(ndata,_) in zip(train_loader,noise_train_loader):\n",
    "    print(data[0],ndata[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs,encoder,decoder,dataLoader,noise_dataLoader,loss_fn,optimizer,model_save_path=\"./model_\"):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for (data,_),(noise_data,_) in zip(dataLoader,noise_dataLoader):\n",
    "            data = data.to(torch.float32)\n",
    "            noise_data = noise_data.to(torch.float32)\n",
    "            optimizer.zero_grad()\n",
    "            encoded = encoder(noise_data)\n",
    "            decoded = decoder(encoded)\n",
    "            loss = loss_fn(decoded, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        print('epoch: %d, train loss: %f' % (epoch, loss.data))\n",
    "    print('Saving NN to %s' % model_save_path)\n",
    "    torch.save(encoder.state_dict(), model_save_path + \"Encoder.pth\")\n",
    "    torch.save(decoder.state_dict(), model_save_path + \"Decoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder,decoder,df,model_path=\"./model_\"):\n",
    "    x = df.values\n",
    "\n",
    "    encoder.load_state_dict(torch.load(model_path + \"Encoder.pth\"))\n",
    "    decoder.load_state_dict(torch.load(model_path + \"Decoder.pth\"))\n",
    "    # put model into test mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        encoded = encoder(torch.tensor(x,dtype=torch.float32))\n",
    "        decoded = decoder(encoded)\n",
    "        \n",
    "        print(decoded.shape)\n",
    "        denoised = pd.DataFrame(decoded.numpy().round())\n",
    "        return denoised\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train loss: 0.250175\n",
      "epoch: 2, train loss: 0.248338\n",
      "epoch: 3, train loss: 0.246113\n",
      "epoch: 4, train loss: 0.243614\n",
      "epoch: 5, train loss: 0.241902\n",
      "epoch: 6, train loss: 0.241698\n",
      "epoch: 7, train loss: 0.241550\n",
      "epoch: 8, train loss: 0.241415\n",
      "epoch: 9, train loss: 0.241269\n",
      "epoch: 10, train loss: 0.241131\n",
      "epoch: 11, train loss: 0.240982\n",
      "epoch: 12, train loss: 0.240657\n",
      "epoch: 13, train loss: 0.240527\n",
      "epoch: 14, train loss: 0.239707\n",
      "epoch: 15, train loss: 0.238748\n",
      "epoch: 16, train loss: 0.237462\n",
      "epoch: 17, train loss: 0.237005\n",
      "epoch: 18, train loss: 0.236662\n",
      "epoch: 19, train loss: 0.236228\n",
      "epoch: 20, train loss: 0.235859\n",
      "epoch: 21, train loss: 0.235709\n",
      "epoch: 22, train loss: 0.235667\n",
      "epoch: 23, train loss: 0.235625\n",
      "epoch: 24, train loss: 0.235589\n",
      "epoch: 25, train loss: 0.235590\n",
      "epoch: 26, train loss: 0.235561\n",
      "epoch: 27, train loss: 0.235553\n",
      "epoch: 28, train loss: 0.235552\n",
      "epoch: 29, train loss: 0.235544\n",
      "epoch: 30, train loss: 0.235536\n",
      "epoch: 31, train loss: 0.235534\n",
      "epoch: 32, train loss: 0.235577\n",
      "epoch: 33, train loss: 0.235529\n",
      "epoch: 34, train loss: 0.235534\n",
      "epoch: 35, train loss: 0.235531\n",
      "epoch: 36, train loss: 0.235527\n",
      "epoch: 37, train loss: 0.235547\n",
      "epoch: 38, train loss: 0.235528\n",
      "epoch: 39, train loss: 0.235526\n",
      "epoch: 40, train loss: 0.235523\n",
      "epoch: 41, train loss: 0.229786\n",
      "epoch: 42, train loss: 0.229638\n",
      "epoch: 43, train loss: 0.229600\n",
      "epoch: 44, train loss: 0.229607\n",
      "epoch: 45, train loss: 0.229624\n",
      "epoch: 46, train loss: 0.229608\n",
      "epoch: 47, train loss: 0.229600\n",
      "epoch: 48, train loss: 0.229603\n",
      "epoch: 49, train loss: 0.229583\n",
      "epoch: 50, train loss: 0.229610\n",
      "epoch: 51, train loss: 0.229592\n",
      "epoch: 52, train loss: 0.229599\n",
      "epoch: 53, train loss: 0.229597\n",
      "epoch: 54, train loss: 0.229594\n",
      "epoch: 55, train loss: 0.229607\n",
      "epoch: 56, train loss: 0.229622\n",
      "epoch: 57, train loss: 0.229604\n",
      "epoch: 58, train loss: 0.229599\n",
      "epoch: 59, train loss: 0.229582\n",
      "epoch: 60, train loss: 0.229583\n",
      "epoch: 61, train loss: 0.229589\n",
      "epoch: 62, train loss: 0.229590\n",
      "epoch: 63, train loss: 0.229584\n",
      "epoch: 64, train loss: 0.229591\n",
      "epoch: 65, train loss: 0.229584\n",
      "epoch: 66, train loss: 0.229578\n",
      "epoch: 67, train loss: 0.229575\n",
      "epoch: 68, train loss: 0.229612\n",
      "epoch: 69, train loss: 0.229571\n",
      "epoch: 70, train loss: 0.229571\n",
      "epoch: 71, train loss: 0.229568\n",
      "epoch: 72, train loss: 0.229570\n",
      "epoch: 73, train loss: 0.229562\n",
      "epoch: 74, train loss: 0.229550\n",
      "epoch: 75, train loss: 0.229543\n",
      "epoch: 76, train loss: 0.229558\n",
      "epoch: 77, train loss: 0.229539\n",
      "epoch: 78, train loss: 0.229536\n",
      "epoch: 79, train loss: 0.229529\n",
      "epoch: 80, train loss: 0.229527\n",
      "epoch: 81, train loss: 0.229519\n",
      "epoch: 82, train loss: 0.229514\n",
      "epoch: 83, train loss: 0.229513\n",
      "epoch: 84, train loss: 0.229502\n",
      "epoch: 85, train loss: 0.229506\n",
      "epoch: 86, train loss: 0.229500\n",
      "epoch: 87, train loss: 0.229499\n",
      "epoch: 88, train loss: 0.229491\n",
      "epoch: 89, train loss: 0.229488\n",
      "epoch: 90, train loss: 0.229496\n",
      "epoch: 91, train loss: 0.229499\n",
      "epoch: 92, train loss: 0.229488\n",
      "epoch: 93, train loss: 0.229497\n",
      "epoch: 94, train loss: 0.229489\n",
      "epoch: 95, train loss: 0.229484\n",
      "epoch: 96, train loss: 0.229486\n",
      "epoch: 97, train loss: 0.229484\n",
      "epoch: 98, train loss: 0.229512\n",
      "epoch: 99, train loss: 0.229483\n",
      "epoch: 100, train loss: 0.229482\n",
      "epoch: 101, train loss: 0.229480\n",
      "epoch: 102, train loss: 0.229481\n",
      "epoch: 103, train loss: 0.229480\n",
      "epoch: 104, train loss: 0.229482\n",
      "epoch: 105, train loss: 0.229480\n",
      "epoch: 106, train loss: 0.229482\n",
      "epoch: 107, train loss: 0.229479\n",
      "epoch: 108, train loss: 0.229479\n",
      "epoch: 109, train loss: 0.229479\n",
      "epoch: 110, train loss: 0.229478\n",
      "epoch: 111, train loss: 0.229479\n",
      "epoch: 112, train loss: 0.229482\n",
      "epoch: 113, train loss: 0.229478\n",
      "epoch: 114, train loss: 0.229477\n",
      "epoch: 115, train loss: 0.229477\n",
      "epoch: 116, train loss: 0.229478\n",
      "epoch: 117, train loss: 0.229479\n",
      "epoch: 118, train loss: 0.229477\n",
      "epoch: 119, train loss: 0.229476\n",
      "epoch: 120, train loss: 0.229477\n",
      "epoch: 121, train loss: 0.229478\n",
      "epoch: 122, train loss: 0.229488\n",
      "epoch: 123, train loss: 0.229478\n",
      "epoch: 124, train loss: 0.229477\n",
      "epoch: 125, train loss: 0.229478\n",
      "epoch: 126, train loss: 0.229477\n",
      "epoch: 127, train loss: 0.229477\n",
      "epoch: 128, train loss: 0.229476\n",
      "epoch: 129, train loss: 0.229477\n",
      "epoch: 130, train loss: 0.229476\n",
      "epoch: 131, train loss: 0.229478\n",
      "epoch: 132, train loss: 0.229477\n",
      "epoch: 133, train loss: 0.229477\n",
      "epoch: 134, train loss: 0.229477\n",
      "epoch: 135, train loss: 0.229477\n",
      "epoch: 136, train loss: 0.229477\n",
      "epoch: 137, train loss: 0.229477\n",
      "epoch: 138, train loss: 0.229477\n",
      "epoch: 139, train loss: 0.229478\n",
      "epoch: 140, train loss: 0.229477\n",
      "epoch: 141, train loss: 0.229477\n",
      "epoch: 142, train loss: 0.229478\n",
      "epoch: 143, train loss: 0.229477\n",
      "epoch: 144, train loss: 0.229478\n",
      "epoch: 145, train loss: 0.229477\n",
      "epoch: 146, train loss: 0.229478\n",
      "epoch: 147, train loss: 0.229477\n",
      "epoch: 148, train loss: 0.229477\n",
      "epoch: 149, train loss: 0.229479\n",
      "epoch: 150, train loss: 0.229482\n",
      "epoch: 151, train loss: 0.229477\n",
      "epoch: 152, train loss: 0.229477\n",
      "epoch: 153, train loss: 0.229476\n",
      "epoch: 154, train loss: 0.229477\n",
      "epoch: 155, train loss: 0.229475\n",
      "epoch: 156, train loss: 0.229476\n",
      "epoch: 157, train loss: 0.229481\n",
      "epoch: 158, train loss: 0.229476\n",
      "epoch: 159, train loss: 0.229478\n",
      "epoch: 160, train loss: 0.229477\n",
      "epoch: 161, train loss: 0.229478\n",
      "epoch: 162, train loss: 0.229479\n",
      "epoch: 163, train loss: 0.229479\n",
      "epoch: 164, train loss: 0.229477\n",
      "epoch: 165, train loss: 0.229475\n",
      "epoch: 166, train loss: 0.229476\n",
      "epoch: 167, train loss: 0.229475\n",
      "epoch: 168, train loss: 0.229477\n",
      "epoch: 169, train loss: 0.229480\n",
      "epoch: 170, train loss: 0.229476\n",
      "epoch: 171, train loss: 0.229478\n",
      "epoch: 172, train loss: 0.229476\n",
      "epoch: 173, train loss: 0.229478\n",
      "epoch: 174, train loss: 0.229477\n",
      "epoch: 175, train loss: 0.229477\n",
      "epoch: 176, train loss: 0.229478\n",
      "epoch: 177, train loss: 0.229475\n",
      "epoch: 178, train loss: 0.229475\n",
      "epoch: 179, train loss: 0.229476\n",
      "epoch: 180, train loss: 0.229478\n",
      "epoch: 181, train loss: 0.229478\n",
      "epoch: 182, train loss: 0.229475\n",
      "epoch: 183, train loss: 0.229482\n",
      "epoch: 184, train loss: 0.229481\n",
      "epoch: 185, train loss: 0.229475\n",
      "epoch: 186, train loss: 0.229477\n",
      "epoch: 187, train loss: 0.229475\n",
      "epoch: 188, train loss: 0.229476\n",
      "epoch: 189, train loss: 0.229475\n",
      "epoch: 190, train loss: 0.229477\n",
      "epoch: 191, train loss: 0.229475\n",
      "epoch: 192, train loss: 0.229480\n",
      "epoch: 193, train loss: 0.229475\n",
      "epoch: 194, train loss: 0.229479\n",
      "epoch: 195, train loss: 0.229475\n",
      "epoch: 196, train loss: 0.229475\n",
      "epoch: 197, train loss: 0.229477\n",
      "epoch: 198, train loss: 0.229475\n",
      "epoch: 199, train loss: 0.229476\n",
      "epoch: 200, train loss: 0.229477\n",
      "epoch: 201, train loss: 0.229477\n",
      "epoch: 202, train loss: 0.229478\n",
      "epoch: 203, train loss: 0.229476\n",
      "epoch: 204, train loss: 0.229475\n",
      "epoch: 205, train loss: 0.229476\n",
      "epoch: 206, train loss: 0.229476\n",
      "epoch: 207, train loss: 0.229477\n",
      "epoch: 208, train loss: 0.229475\n",
      "epoch: 209, train loss: 0.229475\n",
      "epoch: 210, train loss: 0.229475\n",
      "epoch: 211, train loss: 0.229475\n",
      "epoch: 212, train loss: 0.229475\n",
      "epoch: 213, train loss: 0.229476\n",
      "epoch: 214, train loss: 0.229474\n",
      "epoch: 215, train loss: 0.229474\n",
      "epoch: 216, train loss: 0.229475\n",
      "epoch: 217, train loss: 0.229474\n",
      "epoch: 218, train loss: 0.229474\n",
      "epoch: 219, train loss: 0.229475\n",
      "epoch: 220, train loss: 0.229475\n",
      "epoch: 221, train loss: 0.229476\n",
      "epoch: 222, train loss: 0.229476\n",
      "epoch: 223, train loss: 0.229475\n",
      "epoch: 224, train loss: 0.229476\n",
      "epoch: 225, train loss: 0.229475\n",
      "epoch: 226, train loss: 0.229475\n",
      "epoch: 227, train loss: 0.229476\n",
      "epoch: 228, train loss: 0.229476\n",
      "epoch: 229, train loss: 0.229475\n",
      "epoch: 230, train loss: 0.229475\n",
      "epoch: 231, train loss: 0.229475\n",
      "epoch: 232, train loss: 0.229477\n",
      "epoch: 233, train loss: 0.229477\n",
      "epoch: 234, train loss: 0.229477\n",
      "epoch: 235, train loss: 0.229475\n",
      "epoch: 236, train loss: 0.229475\n",
      "epoch: 237, train loss: 0.229475\n",
      "epoch: 238, train loss: 0.229475\n",
      "epoch: 239, train loss: 0.229475\n",
      "epoch: 240, train loss: 0.229475\n",
      "epoch: 241, train loss: 0.229476\n",
      "epoch: 242, train loss: 0.229475\n",
      "epoch: 243, train loss: 0.229475\n",
      "epoch: 244, train loss: 0.229476\n",
      "epoch: 245, train loss: 0.229476\n",
      "epoch: 246, train loss: 0.229477\n",
      "epoch: 247, train loss: 0.229476\n",
      "epoch: 248, train loss: 0.229475\n",
      "epoch: 249, train loss: 0.229476\n",
      "epoch: 250, train loss: 0.229475\n",
      "epoch: 251, train loss: 0.229475\n",
      "epoch: 252, train loss: 0.229475\n",
      "epoch: 253, train loss: 0.229475\n",
      "epoch: 254, train loss: 0.229475\n",
      "epoch: 255, train loss: 0.229474\n",
      "epoch: 256, train loss: 0.229475\n",
      "epoch: 257, train loss: 0.229476\n",
      "epoch: 258, train loss: 0.229475\n",
      "epoch: 259, train loss: 0.229476\n",
      "epoch: 260, train loss: 0.229475\n",
      "epoch: 261, train loss: 0.229475\n",
      "epoch: 262, train loss: 0.229478\n",
      "epoch: 263, train loss: 0.229475\n",
      "epoch: 264, train loss: 0.229476\n",
      "epoch: 265, train loss: 0.229476\n",
      "epoch: 266, train loss: 0.229476\n",
      "epoch: 267, train loss: 0.229477\n",
      "epoch: 268, train loss: 0.229476\n",
      "epoch: 269, train loss: 0.229476\n",
      "epoch: 270, train loss: 0.229475\n",
      "epoch: 271, train loss: 0.229476\n",
      "epoch: 272, train loss: 0.229476\n",
      "epoch: 273, train loss: 0.229476\n",
      "epoch: 274, train loss: 0.229476\n",
      "epoch: 275, train loss: 0.229476\n",
      "epoch: 276, train loss: 0.229476\n",
      "epoch: 277, train loss: 0.229475\n",
      "epoch: 278, train loss: 0.229476\n",
      "epoch: 279, train loss: 0.229475\n",
      "epoch: 280, train loss: 0.229477\n",
      "epoch: 281, train loss: 0.229476\n",
      "epoch: 282, train loss: 0.229475\n",
      "epoch: 283, train loss: 0.229476\n",
      "epoch: 284, train loss: 0.229476\n",
      "epoch: 285, train loss: 0.229476\n",
      "epoch: 286, train loss: 0.229477\n",
      "epoch: 287, train loss: 0.229476\n",
      "epoch: 288, train loss: 0.229476\n",
      "epoch: 289, train loss: 0.229476\n",
      "epoch: 290, train loss: 0.229476\n",
      "epoch: 291, train loss: 0.229476\n",
      "epoch: 292, train loss: 0.229476\n",
      "epoch: 293, train loss: 0.229475\n",
      "epoch: 294, train loss: 0.229476\n",
      "epoch: 295, train loss: 0.229475\n",
      "epoch: 296, train loss: 0.229475\n",
      "epoch: 297, train loss: 0.229476\n",
      "epoch: 298, train loss: 0.229475\n",
      "epoch: 299, train loss: 0.229475\n",
      "epoch: 300, train loss: 0.229475\n",
      "epoch: 301, train loss: 0.229475\n",
      "epoch: 302, train loss: 0.229476\n",
      "epoch: 303, train loss: 0.229478\n",
      "epoch: 304, train loss: 0.229476\n",
      "epoch: 305, train loss: 0.229476\n",
      "epoch: 306, train loss: 0.229476\n",
      "epoch: 307, train loss: 0.229475\n",
      "epoch: 308, train loss: 0.229476\n",
      "epoch: 309, train loss: 0.229476\n",
      "epoch: 310, train loss: 0.229476\n",
      "epoch: 311, train loss: 0.229476\n",
      "epoch: 312, train loss: 0.229475\n",
      "epoch: 313, train loss: 0.229477\n",
      "epoch: 314, train loss: 0.229476\n",
      "epoch: 315, train loss: 0.229475\n",
      "epoch: 316, train loss: 0.229476\n",
      "epoch: 317, train loss: 0.229475\n",
      "epoch: 318, train loss: 0.229475\n",
      "epoch: 319, train loss: 0.229475\n",
      "epoch: 320, train loss: 0.229476\n",
      "epoch: 321, train loss: 0.229475\n",
      "epoch: 322, train loss: 0.229476\n",
      "epoch: 323, train loss: 0.229476\n",
      "epoch: 324, train loss: 0.229476\n",
      "epoch: 325, train loss: 0.229476\n",
      "epoch: 326, train loss: 0.229475\n",
      "epoch: 327, train loss: 0.229476\n",
      "epoch: 328, train loss: 0.229475\n",
      "epoch: 329, train loss: 0.229476\n",
      "epoch: 330, train loss: 0.229476\n",
      "epoch: 331, train loss: 0.229476\n",
      "epoch: 332, train loss: 0.229476\n",
      "epoch: 333, train loss: 0.229477\n",
      "epoch: 334, train loss: 0.229476\n",
      "epoch: 335, train loss: 0.229476\n",
      "epoch: 336, train loss: 0.229476\n",
      "epoch: 337, train loss: 0.229476\n",
      "epoch: 338, train loss: 0.229477\n",
      "epoch: 339, train loss: 0.229476\n",
      "epoch: 340, train loss: 0.229476\n",
      "epoch: 341, train loss: 0.229477\n",
      "epoch: 342, train loss: 0.229476\n",
      "epoch: 343, train loss: 0.229477\n",
      "epoch: 344, train loss: 0.229476\n",
      "epoch: 345, train loss: 0.229477\n",
      "epoch: 346, train loss: 0.229476\n",
      "epoch: 347, train loss: 0.229476\n",
      "epoch: 348, train loss: 0.229476\n",
      "epoch: 349, train loss: 0.229476\n",
      "epoch: 350, train loss: 0.229476\n",
      "epoch: 351, train loss: 0.229476\n",
      "epoch: 352, train loss: 0.229477\n",
      "epoch: 353, train loss: 0.229476\n",
      "epoch: 354, train loss: 0.229476\n",
      "epoch: 355, train loss: 0.229477\n",
      "epoch: 356, train loss: 0.229477\n",
      "epoch: 357, train loss: 0.229476\n",
      "epoch: 358, train loss: 0.229476\n",
      "epoch: 359, train loss: 0.229476\n",
      "epoch: 360, train loss: 0.229476\n",
      "epoch: 361, train loss: 0.229476\n",
      "epoch: 362, train loss: 0.229476\n",
      "epoch: 363, train loss: 0.229476\n",
      "epoch: 364, train loss: 0.229475\n",
      "epoch: 365, train loss: 0.229476\n",
      "epoch: 366, train loss: 0.229476\n",
      "epoch: 367, train loss: 0.229475\n",
      "epoch: 368, train loss: 0.229476\n",
      "epoch: 369, train loss: 0.229476\n",
      "epoch: 370, train loss: 0.229475\n",
      "epoch: 371, train loss: 0.229477\n",
      "epoch: 372, train loss: 0.229477\n",
      "epoch: 373, train loss: 0.229475\n",
      "epoch: 374, train loss: 0.229477\n",
      "epoch: 375, train loss: 0.229579\n",
      "epoch: 376, train loss: 0.229475\n",
      "epoch: 377, train loss: 0.229477\n",
      "epoch: 378, train loss: 0.229476\n",
      "epoch: 379, train loss: 0.229476\n",
      "epoch: 380, train loss: 0.229475\n",
      "epoch: 381, train loss: 0.229476\n",
      "epoch: 382, train loss: 0.229476\n",
      "epoch: 383, train loss: 0.229476\n",
      "epoch: 384, train loss: 0.229476\n",
      "epoch: 385, train loss: 0.229476\n",
      "epoch: 386, train loss: 0.229488\n",
      "epoch: 387, train loss: 0.229476\n",
      "epoch: 388, train loss: 0.229476\n",
      "epoch: 389, train loss: 0.229476\n",
      "epoch: 390, train loss: 0.229476\n",
      "epoch: 391, train loss: 0.229477\n",
      "epoch: 392, train loss: 0.229476\n",
      "epoch: 393, train loss: 0.229476\n",
      "epoch: 394, train loss: 0.229476\n",
      "epoch: 395, train loss: 0.229476\n",
      "epoch: 396, train loss: 0.229475\n",
      "epoch: 397, train loss: 0.229476\n",
      "epoch: 398, train loss: 0.229476\n",
      "epoch: 399, train loss: 0.229476\n",
      "epoch: 400, train loss: 0.229476\n",
      "epoch: 401, train loss: 0.229476\n",
      "epoch: 402, train loss: 0.229475\n",
      "epoch: 403, train loss: 0.229476\n",
      "epoch: 404, train loss: 0.229476\n",
      "epoch: 405, train loss: 0.229477\n",
      "epoch: 406, train loss: 0.229475\n",
      "epoch: 407, train loss: 0.229475\n",
      "epoch: 408, train loss: 0.229475\n",
      "epoch: 409, train loss: 0.229476\n",
      "epoch: 410, train loss: 0.229475\n",
      "epoch: 411, train loss: 0.229476\n",
      "epoch: 412, train loss: 0.229477\n",
      "epoch: 413, train loss: 0.229477\n",
      "epoch: 414, train loss: 0.229476\n",
      "epoch: 415, train loss: 0.229477\n",
      "epoch: 416, train loss: 0.229474\n",
      "epoch: 417, train loss: 0.229475\n",
      "epoch: 418, train loss: 0.229476\n",
      "epoch: 419, train loss: 0.229477\n",
      "epoch: 420, train loss: 0.229476\n",
      "epoch: 421, train loss: 0.229476\n",
      "epoch: 422, train loss: 0.229631\n",
      "epoch: 423, train loss: 0.229476\n",
      "epoch: 424, train loss: 0.229476\n",
      "epoch: 425, train loss: 0.229476\n",
      "epoch: 426, train loss: 0.229476\n",
      "epoch: 427, train loss: 0.229476\n",
      "epoch: 428, train loss: 0.229475\n",
      "epoch: 429, train loss: 0.229475\n",
      "epoch: 430, train loss: 0.229476\n",
      "epoch: 431, train loss: 0.229477\n",
      "epoch: 432, train loss: 0.229477\n",
      "epoch: 433, train loss: 0.229477\n",
      "epoch: 434, train loss: 0.229476\n",
      "epoch: 435, train loss: 0.229477\n",
      "epoch: 436, train loss: 0.229476\n",
      "epoch: 437, train loss: 0.229476\n",
      "epoch: 438, train loss: 0.229476\n",
      "epoch: 439, train loss: 0.229476\n",
      "epoch: 440, train loss: 0.229477\n",
      "epoch: 441, train loss: 0.229476\n",
      "epoch: 442, train loss: 0.229478\n",
      "epoch: 443, train loss: 0.229477\n",
      "epoch: 444, train loss: 0.229476\n",
      "epoch: 445, train loss: 0.229476\n",
      "epoch: 446, train loss: 0.229476\n",
      "epoch: 447, train loss: 0.229476\n",
      "epoch: 448, train loss: 0.229476\n",
      "epoch: 449, train loss: 0.229477\n",
      "epoch: 450, train loss: 0.229476\n",
      "epoch: 451, train loss: 0.229475\n",
      "epoch: 452, train loss: 0.229476\n",
      "epoch: 453, train loss: 0.229476\n",
      "epoch: 454, train loss: 0.229475\n",
      "epoch: 455, train loss: 0.229476\n",
      "epoch: 456, train loss: 0.229476\n",
      "epoch: 457, train loss: 0.229476\n",
      "epoch: 458, train loss: 0.229475\n",
      "epoch: 459, train loss: 0.229474\n",
      "epoch: 460, train loss: 0.229475\n",
      "epoch: 461, train loss: 0.229476\n",
      "epoch: 462, train loss: 0.229474\n",
      "epoch: 463, train loss: 0.229475\n",
      "epoch: 464, train loss: 0.229476\n",
      "epoch: 465, train loss: 0.229475\n",
      "epoch: 466, train loss: 0.229476\n",
      "epoch: 467, train loss: 0.229476\n",
      "epoch: 468, train loss: 0.229477\n",
      "epoch: 469, train loss: 0.229475\n",
      "epoch: 470, train loss: 0.229476\n",
      "epoch: 471, train loss: 0.229477\n",
      "epoch: 472, train loss: 0.229476\n",
      "epoch: 473, train loss: 0.229475\n",
      "epoch: 474, train loss: 0.229477\n",
      "epoch: 475, train loss: 0.229476\n",
      "epoch: 476, train loss: 0.229474\n",
      "epoch: 477, train loss: 0.229476\n",
      "epoch: 478, train loss: 0.229475\n",
      "epoch: 479, train loss: 0.229475\n",
      "epoch: 480, train loss: 0.229475\n",
      "epoch: 481, train loss: 0.229475\n",
      "epoch: 482, train loss: 0.229476\n",
      "epoch: 483, train loss: 0.229476\n",
      "epoch: 484, train loss: 0.229475\n",
      "epoch: 485, train loss: 0.229476\n",
      "epoch: 486, train loss: 0.229475\n",
      "epoch: 487, train loss: 0.229474\n",
      "epoch: 488, train loss: 0.229475\n",
      "epoch: 489, train loss: 0.229475\n",
      "epoch: 490, train loss: 0.229475\n",
      "epoch: 491, train loss: 0.229476\n",
      "epoch: 492, train loss: 0.229476\n",
      "epoch: 493, train loss: 0.229477\n",
      "epoch: 494, train loss: 0.229476\n",
      "epoch: 495, train loss: 0.229475\n",
      "epoch: 496, train loss: 0.229476\n",
      "epoch: 497, train loss: 0.229476\n",
      "epoch: 498, train loss: 0.229475\n",
      "epoch: 499, train loss: 0.229475\n",
      "epoch: 500, train loss: 0.229476\n",
      "epoch: 501, train loss: 0.229475\n",
      "epoch: 502, train loss: 0.229475\n",
      "epoch: 503, train loss: 0.229475\n",
      "epoch: 504, train loss: 0.229474\n",
      "epoch: 505, train loss: 0.229476\n",
      "epoch: 506, train loss: 0.229475\n",
      "epoch: 507, train loss: 0.229475\n",
      "epoch: 508, train loss: 0.229475\n",
      "epoch: 509, train loss: 0.229476\n",
      "epoch: 510, train loss: 0.229476\n",
      "epoch: 511, train loss: 0.229475\n",
      "epoch: 512, train loss: 0.229476\n",
      "epoch: 513, train loss: 0.229476\n",
      "epoch: 514, train loss: 0.229477\n",
      "epoch: 515, train loss: 0.229476\n",
      "epoch: 516, train loss: 0.229475\n",
      "epoch: 517, train loss: 0.229475\n",
      "epoch: 518, train loss: 0.229475\n",
      "epoch: 519, train loss: 0.229476\n",
      "epoch: 520, train loss: 0.229474\n",
      "epoch: 521, train loss: 0.229475\n",
      "epoch: 522, train loss: 0.229475\n",
      "epoch: 523, train loss: 0.229475\n",
      "epoch: 524, train loss: 0.229475\n",
      "epoch: 525, train loss: 0.229474\n",
      "epoch: 526, train loss: 0.229475\n",
      "epoch: 527, train loss: 0.229475\n",
      "epoch: 528, train loss: 0.229475\n",
      "epoch: 529, train loss: 0.229476\n",
      "epoch: 530, train loss: 0.229475\n",
      "epoch: 531, train loss: 0.229477\n",
      "epoch: 532, train loss: 0.229475\n",
      "epoch: 533, train loss: 0.229475\n",
      "epoch: 534, train loss: 0.229475\n",
      "epoch: 535, train loss: 0.229475\n",
      "epoch: 536, train loss: 0.229475\n",
      "epoch: 537, train loss: 0.229475\n",
      "epoch: 538, train loss: 0.229474\n",
      "epoch: 539, train loss: 0.229476\n",
      "epoch: 540, train loss: 0.229475\n",
      "epoch: 541, train loss: 0.229475\n",
      "epoch: 542, train loss: 0.229474\n",
      "epoch: 543, train loss: 0.229475\n",
      "epoch: 544, train loss: 0.229475\n",
      "epoch: 545, train loss: 0.229476\n",
      "epoch: 546, train loss: 0.229476\n",
      "epoch: 547, train loss: 0.229475\n",
      "epoch: 548, train loss: 0.229475\n",
      "epoch: 549, train loss: 0.229476\n",
      "epoch: 550, train loss: 0.229475\n",
      "epoch: 551, train loss: 0.229476\n",
      "epoch: 552, train loss: 0.229475\n",
      "epoch: 553, train loss: 0.229475\n",
      "epoch: 554, train loss: 0.229475\n",
      "epoch: 555, train loss: 0.229476\n",
      "epoch: 556, train loss: 0.229475\n",
      "epoch: 557, train loss: 0.229475\n",
      "epoch: 558, train loss: 0.229475\n",
      "epoch: 559, train loss: 0.229475\n",
      "epoch: 560, train loss: 0.229476\n",
      "epoch: 561, train loss: 0.229476\n",
      "epoch: 562, train loss: 0.229476\n",
      "epoch: 563, train loss: 0.229475\n",
      "epoch: 564, train loss: 0.229474\n",
      "epoch: 565, train loss: 0.229476\n",
      "epoch: 566, train loss: 0.229475\n",
      "epoch: 567, train loss: 0.229475\n",
      "epoch: 568, train loss: 0.229475\n",
      "epoch: 569, train loss: 0.229474\n",
      "epoch: 570, train loss: 0.229476\n",
      "epoch: 571, train loss: 0.229475\n",
      "epoch: 572, train loss: 0.229474\n",
      "epoch: 573, train loss: 0.229474\n",
      "epoch: 574, train loss: 0.229474\n",
      "epoch: 575, train loss: 0.229475\n",
      "epoch: 576, train loss: 0.229477\n",
      "epoch: 577, train loss: 0.229475\n",
      "epoch: 578, train loss: 0.229475\n",
      "epoch: 579, train loss: 0.229508\n",
      "epoch: 580, train loss: 0.229476\n",
      "epoch: 581, train loss: 0.229477\n",
      "epoch: 582, train loss: 0.229476\n",
      "epoch: 583, train loss: 0.229475\n",
      "epoch: 584, train loss: 0.229475\n",
      "epoch: 585, train loss: 0.229476\n",
      "epoch: 586, train loss: 0.229475\n",
      "epoch: 587, train loss: 0.229475\n",
      "epoch: 588, train loss: 0.229475\n",
      "epoch: 589, train loss: 0.229475\n",
      "epoch: 590, train loss: 0.229475\n",
      "epoch: 591, train loss: 0.229475\n",
      "epoch: 592, train loss: 0.229475\n",
      "epoch: 593, train loss: 0.229475\n",
      "epoch: 594, train loss: 0.229475\n",
      "epoch: 595, train loss: 0.229475\n",
      "epoch: 596, train loss: 0.229475\n",
      "epoch: 597, train loss: 0.229475\n",
      "epoch: 598, train loss: 0.229476\n",
      "epoch: 599, train loss: 0.229476\n",
      "epoch: 600, train loss: 0.229475\n",
      "epoch: 601, train loss: 0.229475\n",
      "epoch: 602, train loss: 0.229477\n",
      "epoch: 603, train loss: 0.229475\n",
      "epoch: 604, train loss: 0.229475\n",
      "epoch: 605, train loss: 0.229475\n",
      "epoch: 606, train loss: 0.229474\n",
      "epoch: 607, train loss: 0.229475\n",
      "epoch: 608, train loss: 0.229474\n",
      "epoch: 609, train loss: 0.229475\n",
      "epoch: 610, train loss: 0.229476\n",
      "epoch: 611, train loss: 0.229474\n",
      "epoch: 612, train loss: 0.229475\n",
      "epoch: 613, train loss: 0.229475\n",
      "epoch: 614, train loss: 0.229476\n",
      "epoch: 615, train loss: 0.229476\n",
      "epoch: 616, train loss: 0.229474\n",
      "epoch: 617, train loss: 0.229476\n",
      "epoch: 618, train loss: 0.229475\n",
      "epoch: 619, train loss: 0.229476\n",
      "epoch: 620, train loss: 0.229476\n",
      "epoch: 621, train loss: 0.229476\n",
      "epoch: 622, train loss: 0.229474\n",
      "epoch: 623, train loss: 0.229475\n",
      "epoch: 624, train loss: 0.229476\n",
      "epoch: 625, train loss: 0.229475\n",
      "epoch: 626, train loss: 0.229475\n",
      "epoch: 627, train loss: 0.229476\n",
      "epoch: 628, train loss: 0.229475\n",
      "epoch: 629, train loss: 0.229475\n",
      "epoch: 630, train loss: 0.229475\n",
      "epoch: 631, train loss: 0.229475\n",
      "epoch: 632, train loss: 0.229477\n",
      "epoch: 633, train loss: 0.229476\n",
      "epoch: 634, train loss: 0.229474\n",
      "epoch: 635, train loss: 0.229476\n",
      "epoch: 636, train loss: 0.229474\n",
      "epoch: 637, train loss: 0.229475\n",
      "epoch: 638, train loss: 0.229476\n",
      "epoch: 639, train loss: 0.229475\n",
      "epoch: 640, train loss: 0.229474\n",
      "epoch: 641, train loss: 0.229475\n",
      "epoch: 642, train loss: 0.229475\n",
      "epoch: 643, train loss: 0.229477\n",
      "epoch: 644, train loss: 0.229475\n",
      "epoch: 645, train loss: 0.229477\n",
      "epoch: 646, train loss: 0.229477\n",
      "epoch: 647, train loss: 0.229476\n",
      "epoch: 648, train loss: 0.229475\n",
      "epoch: 649, train loss: 0.229475\n",
      "epoch: 650, train loss: 0.229474\n",
      "epoch: 651, train loss: 0.229477\n",
      "epoch: 652, train loss: 0.229474\n",
      "epoch: 653, train loss: 0.229476\n",
      "epoch: 654, train loss: 0.229476\n",
      "epoch: 655, train loss: 0.229475\n",
      "epoch: 656, train loss: 0.229477\n",
      "epoch: 657, train loss: 0.229474\n",
      "epoch: 658, train loss: 0.229474\n",
      "epoch: 659, train loss: 0.229475\n",
      "epoch: 660, train loss: 0.229475\n",
      "epoch: 661, train loss: 0.229475\n",
      "epoch: 662, train loss: 0.229475\n",
      "epoch: 663, train loss: 0.229474\n",
      "epoch: 664, train loss: 0.229475\n",
      "epoch: 665, train loss: 0.229475\n",
      "epoch: 666, train loss: 0.229476\n",
      "epoch: 667, train loss: 0.229476\n",
      "epoch: 668, train loss: 0.229477\n",
      "epoch: 669, train loss: 0.229477\n",
      "epoch: 670, train loss: 0.229476\n",
      "epoch: 671, train loss: 0.229475\n",
      "epoch: 672, train loss: 0.229476\n",
      "epoch: 673, train loss: 0.229475\n",
      "epoch: 674, train loss: 0.229475\n",
      "epoch: 675, train loss: 0.229477\n",
      "epoch: 676, train loss: 0.229476\n",
      "epoch: 677, train loss: 0.229475\n",
      "epoch: 678, train loss: 0.229476\n",
      "epoch: 679, train loss: 0.229475\n",
      "epoch: 680, train loss: 0.229474\n",
      "epoch: 681, train loss: 0.229477\n",
      "epoch: 682, train loss: 0.229476\n",
      "epoch: 683, train loss: 0.229475\n",
      "epoch: 684, train loss: 0.229477\n",
      "epoch: 685, train loss: 0.229475\n",
      "epoch: 686, train loss: 0.229475\n",
      "epoch: 687, train loss: 0.229476\n",
      "epoch: 688, train loss: 0.229475\n",
      "epoch: 689, train loss: 0.229475\n",
      "epoch: 690, train loss: 0.229476\n",
      "epoch: 691, train loss: 0.229475\n",
      "epoch: 692, train loss: 0.229475\n",
      "epoch: 693, train loss: 0.229475\n",
      "epoch: 694, train loss: 0.229476\n",
      "epoch: 695, train loss: 0.229475\n",
      "epoch: 696, train loss: 0.229476\n",
      "epoch: 697, train loss: 0.229476\n",
      "epoch: 698, train loss: 0.229474\n",
      "epoch: 699, train loss: 0.229476\n",
      "epoch: 700, train loss: 0.229476\n",
      "epoch: 701, train loss: 0.229475\n",
      "epoch: 702, train loss: 0.229475\n",
      "epoch: 703, train loss: 0.229476\n",
      "epoch: 704, train loss: 0.229478\n",
      "epoch: 705, train loss: 0.229476\n",
      "epoch: 706, train loss: 0.229476\n",
      "epoch: 707, train loss: 0.229475\n",
      "epoch: 708, train loss: 0.229477\n",
      "epoch: 709, train loss: 0.229475\n",
      "epoch: 710, train loss: 0.229475\n",
      "epoch: 711, train loss: 0.229475\n",
      "epoch: 712, train loss: 0.229475\n",
      "epoch: 713, train loss: 0.229475\n",
      "epoch: 714, train loss: 0.229476\n",
      "epoch: 715, train loss: 0.229476\n",
      "epoch: 716, train loss: 0.229475\n",
      "epoch: 717, train loss: 0.229475\n",
      "epoch: 718, train loss: 0.229476\n",
      "epoch: 719, train loss: 0.229475\n",
      "epoch: 720, train loss: 0.229478\n",
      "epoch: 721, train loss: 0.229476\n",
      "epoch: 722, train loss: 0.229475\n",
      "epoch: 723, train loss: 0.229475\n",
      "epoch: 724, train loss: 0.229475\n",
      "epoch: 725, train loss: 0.229475\n",
      "epoch: 726, train loss: 0.229475\n",
      "epoch: 727, train loss: 0.229475\n",
      "epoch: 728, train loss: 0.229476\n",
      "epoch: 729, train loss: 0.229476\n",
      "epoch: 730, train loss: 0.229476\n",
      "epoch: 731, train loss: 0.229476\n",
      "epoch: 732, train loss: 0.229477\n",
      "epoch: 733, train loss: 0.229477\n",
      "epoch: 734, train loss: 0.229475\n",
      "epoch: 735, train loss: 0.229475\n",
      "epoch: 736, train loss: 0.229475\n",
      "epoch: 737, train loss: 0.229476\n",
      "epoch: 738, train loss: 0.229476\n",
      "epoch: 739, train loss: 0.229475\n",
      "epoch: 740, train loss: 0.229475\n",
      "epoch: 741, train loss: 0.229479\n",
      "epoch: 742, train loss: 0.229476\n",
      "epoch: 743, train loss: 0.229474\n",
      "epoch: 744, train loss: 0.229475\n",
      "epoch: 745, train loss: 0.229475\n",
      "epoch: 746, train loss: 0.229478\n",
      "epoch: 747, train loss: 0.229477\n",
      "epoch: 748, train loss: 0.229477\n",
      "epoch: 749, train loss: 0.229474\n",
      "epoch: 750, train loss: 0.229476\n",
      "epoch: 751, train loss: 0.229474\n",
      "epoch: 752, train loss: 0.229476\n",
      "epoch: 753, train loss: 0.229477\n",
      "epoch: 754, train loss: 0.229480\n",
      "epoch: 755, train loss: 0.229477\n",
      "epoch: 756, train loss: 0.229476\n",
      "epoch: 757, train loss: 0.229476\n",
      "epoch: 758, train loss: 0.229475\n",
      "epoch: 759, train loss: 0.229476\n",
      "epoch: 760, train loss: 0.229476\n",
      "epoch: 761, train loss: 0.229479\n",
      "epoch: 762, train loss: 0.229477\n",
      "epoch: 763, train loss: 0.229475\n",
      "epoch: 764, train loss: 0.229476\n",
      "epoch: 765, train loss: 0.229476\n",
      "epoch: 766, train loss: 0.229476\n",
      "epoch: 767, train loss: 0.229475\n",
      "epoch: 768, train loss: 0.229476\n",
      "epoch: 769, train loss: 0.229475\n",
      "epoch: 770, train loss: 0.229476\n",
      "epoch: 771, train loss: 0.229476\n",
      "epoch: 772, train loss: 0.229475\n",
      "epoch: 773, train loss: 0.229477\n",
      "epoch: 774, train loss: 0.229475\n",
      "epoch: 775, train loss: 0.229475\n",
      "epoch: 776, train loss: 0.229476\n",
      "epoch: 777, train loss: 0.229479\n",
      "epoch: 778, train loss: 0.229476\n",
      "epoch: 779, train loss: 0.229477\n",
      "epoch: 780, train loss: 0.229476\n",
      "epoch: 781, train loss: 0.229474\n",
      "epoch: 782, train loss: 0.229475\n",
      "epoch: 783, train loss: 0.229475\n",
      "epoch: 784, train loss: 0.229475\n",
      "epoch: 785, train loss: 0.229475\n",
      "epoch: 786, train loss: 0.229476\n",
      "epoch: 787, train loss: 0.229476\n",
      "epoch: 788, train loss: 0.229475\n",
      "epoch: 789, train loss: 0.229475\n",
      "epoch: 790, train loss: 0.229476\n",
      "epoch: 791, train loss: 0.229475\n",
      "epoch: 792, train loss: 0.229474\n",
      "epoch: 793, train loss: 0.229477\n",
      "epoch: 794, train loss: 0.229476\n",
      "epoch: 795, train loss: 0.229476\n",
      "epoch: 796, train loss: 0.229475\n",
      "epoch: 797, train loss: 0.229476\n",
      "epoch: 798, train loss: 0.229477\n",
      "epoch: 799, train loss: 0.229477\n",
      "epoch: 800, train loss: 0.229476\n",
      "epoch: 801, train loss: 0.229474\n",
      "epoch: 802, train loss: 0.229474\n",
      "epoch: 803, train loss: 0.229477\n",
      "epoch: 804, train loss: 0.229476\n",
      "epoch: 805, train loss: 0.229475\n",
      "epoch: 806, train loss: 0.229476\n",
      "epoch: 807, train loss: 0.229475\n",
      "epoch: 808, train loss: 0.229475\n",
      "epoch: 809, train loss: 0.229476\n",
      "epoch: 810, train loss: 0.229476\n",
      "epoch: 811, train loss: 0.229477\n",
      "epoch: 812, train loss: 0.229475\n",
      "epoch: 813, train loss: 0.229477\n",
      "epoch: 814, train loss: 0.229475\n",
      "epoch: 815, train loss: 0.229476\n",
      "epoch: 816, train loss: 0.229475\n",
      "epoch: 817, train loss: 0.229477\n",
      "epoch: 818, train loss: 0.229475\n",
      "epoch: 819, train loss: 0.229477\n",
      "epoch: 820, train loss: 0.229474\n",
      "epoch: 821, train loss: 0.229475\n",
      "epoch: 822, train loss: 0.229475\n",
      "epoch: 823, train loss: 0.229475\n",
      "epoch: 824, train loss: 0.229475\n",
      "epoch: 825, train loss: 0.229477\n",
      "epoch: 826, train loss: 0.229474\n",
      "epoch: 827, train loss: 0.229476\n",
      "epoch: 828, train loss: 0.229474\n",
      "epoch: 829, train loss: 0.229475\n",
      "epoch: 830, train loss: 0.229477\n",
      "epoch: 831, train loss: 0.229476\n",
      "epoch: 832, train loss: 0.229475\n",
      "epoch: 833, train loss: 0.229475\n",
      "epoch: 834, train loss: 0.229474\n",
      "epoch: 835, train loss: 0.229477\n",
      "epoch: 836, train loss: 0.229475\n",
      "epoch: 837, train loss: 0.229476\n",
      "epoch: 838, train loss: 0.229475\n",
      "epoch: 839, train loss: 0.229475\n",
      "epoch: 840, train loss: 0.229475\n",
      "epoch: 841, train loss: 0.229475\n",
      "epoch: 842, train loss: 0.229476\n",
      "epoch: 843, train loss: 0.229475\n",
      "epoch: 844, train loss: 0.229475\n",
      "epoch: 845, train loss: 0.229475\n",
      "epoch: 846, train loss: 0.229475\n",
      "epoch: 847, train loss: 0.229475\n",
      "epoch: 848, train loss: 0.229475\n",
      "epoch: 849, train loss: 0.229476\n",
      "epoch: 850, train loss: 0.229475\n",
      "epoch: 851, train loss: 0.229476\n",
      "epoch: 852, train loss: 0.229476\n",
      "epoch: 853, train loss: 0.229476\n",
      "epoch: 854, train loss: 0.229483\n",
      "epoch: 855, train loss: 0.229494\n",
      "epoch: 856, train loss: 0.229476\n",
      "epoch: 857, train loss: 0.229476\n",
      "epoch: 858, train loss: 0.229475\n",
      "epoch: 859, train loss: 0.229476\n",
      "epoch: 860, train loss: 0.229475\n",
      "epoch: 861, train loss: 0.229475\n",
      "epoch: 862, train loss: 0.229476\n",
      "epoch: 863, train loss: 0.229475\n",
      "epoch: 864, train loss: 0.229475\n",
      "epoch: 865, train loss: 0.229474\n",
      "epoch: 866, train loss: 0.229476\n",
      "epoch: 867, train loss: 0.229474\n",
      "epoch: 868, train loss: 0.229475\n",
      "epoch: 869, train loss: 0.229475\n",
      "epoch: 870, train loss: 0.229475\n",
      "epoch: 871, train loss: 0.229475\n",
      "epoch: 872, train loss: 0.229475\n",
      "epoch: 873, train loss: 0.229475\n",
      "epoch: 874, train loss: 0.229475\n",
      "epoch: 875, train loss: 0.229475\n",
      "epoch: 876, train loss: 0.229475\n",
      "epoch: 877, train loss: 0.229476\n",
      "epoch: 878, train loss: 0.229475\n",
      "epoch: 879, train loss: 0.229474\n",
      "epoch: 880, train loss: 0.229475\n",
      "epoch: 881, train loss: 0.229475\n",
      "epoch: 882, train loss: 0.229475\n",
      "epoch: 883, train loss: 0.229476\n",
      "epoch: 884, train loss: 0.229474\n",
      "epoch: 885, train loss: 0.229477\n",
      "epoch: 886, train loss: 0.229475\n",
      "epoch: 887, train loss: 0.229476\n",
      "epoch: 888, train loss: 0.229475\n",
      "epoch: 889, train loss: 0.229475\n",
      "epoch: 890, train loss: 0.229475\n",
      "epoch: 891, train loss: 0.229476\n",
      "epoch: 892, train loss: 0.229477\n",
      "epoch: 893, train loss: 0.229476\n",
      "epoch: 894, train loss: 0.229475\n",
      "epoch: 895, train loss: 0.229475\n",
      "epoch: 896, train loss: 0.229476\n",
      "epoch: 897, train loss: 0.229476\n",
      "epoch: 898, train loss: 0.229475\n",
      "epoch: 899, train loss: 0.229477\n",
      "epoch: 900, train loss: 0.229475\n",
      "epoch: 901, train loss: 0.229475\n",
      "epoch: 902, train loss: 0.229475\n",
      "epoch: 903, train loss: 0.229476\n",
      "epoch: 904, train loss: 0.229476\n",
      "epoch: 905, train loss: 0.229475\n",
      "epoch: 906, train loss: 0.229475\n",
      "epoch: 907, train loss: 0.229475\n",
      "epoch: 908, train loss: 0.229475\n",
      "epoch: 909, train loss: 0.229477\n",
      "epoch: 910, train loss: 0.229475\n",
      "epoch: 911, train loss: 0.229475\n",
      "epoch: 912, train loss: 0.229474\n",
      "epoch: 913, train loss: 0.229474\n",
      "epoch: 914, train loss: 0.229475\n",
      "epoch: 915, train loss: 0.229474\n",
      "epoch: 916, train loss: 0.229475\n",
      "epoch: 917, train loss: 0.229475\n",
      "epoch: 918, train loss: 0.229475\n",
      "epoch: 919, train loss: 0.229476\n",
      "epoch: 920, train loss: 0.229475\n",
      "epoch: 921, train loss: 0.229475\n",
      "epoch: 922, train loss: 0.229476\n",
      "epoch: 923, train loss: 0.229475\n",
      "epoch: 924, train loss: 0.229475\n",
      "epoch: 925, train loss: 0.229476\n",
      "epoch: 926, train loss: 0.229477\n",
      "epoch: 927, train loss: 0.229475\n",
      "epoch: 928, train loss: 0.229476\n",
      "epoch: 929, train loss: 0.229475\n",
      "epoch: 930, train loss: 0.229475\n",
      "epoch: 931, train loss: 0.229476\n",
      "epoch: 932, train loss: 0.229476\n",
      "epoch: 933, train loss: 0.229475\n",
      "epoch: 934, train loss: 0.229475\n",
      "epoch: 935, train loss: 0.229475\n",
      "epoch: 936, train loss: 0.229475\n",
      "epoch: 937, train loss: 0.229475\n",
      "epoch: 938, train loss: 0.229475\n",
      "epoch: 939, train loss: 0.229475\n",
      "epoch: 940, train loss: 0.229476\n",
      "epoch: 941, train loss: 0.229475\n",
      "epoch: 942, train loss: 0.229476\n",
      "epoch: 943, train loss: 0.229476\n",
      "epoch: 944, train loss: 0.229476\n",
      "epoch: 945, train loss: 0.229475\n",
      "epoch: 946, train loss: 0.229475\n",
      "epoch: 947, train loss: 0.229475\n",
      "epoch: 948, train loss: 0.229476\n",
      "epoch: 949, train loss: 0.229476\n",
      "epoch: 950, train loss: 0.229474\n",
      "epoch: 951, train loss: 0.229477\n",
      "epoch: 952, train loss: 0.229477\n",
      "epoch: 953, train loss: 0.229475\n",
      "epoch: 954, train loss: 0.229476\n",
      "epoch: 955, train loss: 0.229476\n",
      "epoch: 956, train loss: 0.229475\n",
      "epoch: 957, train loss: 0.229475\n",
      "epoch: 958, train loss: 0.229476\n",
      "epoch: 959, train loss: 0.229478\n",
      "epoch: 960, train loss: 0.229475\n",
      "epoch: 961, train loss: 0.229474\n",
      "epoch: 962, train loss: 0.229477\n",
      "epoch: 963, train loss: 0.229475\n",
      "epoch: 964, train loss: 0.229475\n",
      "epoch: 965, train loss: 0.229476\n",
      "epoch: 966, train loss: 0.229477\n",
      "epoch: 967, train loss: 0.229478\n",
      "epoch: 968, train loss: 0.229475\n",
      "epoch: 969, train loss: 0.229475\n",
      "epoch: 970, train loss: 0.229475\n",
      "epoch: 971, train loss: 0.229474\n",
      "epoch: 972, train loss: 0.229475\n",
      "epoch: 973, train loss: 0.229475\n",
      "epoch: 974, train loss: 0.229476\n",
      "epoch: 975, train loss: 0.229474\n",
      "epoch: 976, train loss: 0.229476\n",
      "epoch: 977, train loss: 0.229475\n",
      "epoch: 978, train loss: 0.229475\n",
      "epoch: 979, train loss: 0.229476\n",
      "epoch: 980, train loss: 0.229476\n",
      "epoch: 981, train loss: 0.229474\n",
      "epoch: 982, train loss: 0.229475\n",
      "epoch: 983, train loss: 0.229475\n",
      "epoch: 984, train loss: 0.229475\n",
      "epoch: 985, train loss: 0.229477\n",
      "epoch: 986, train loss: 0.229475\n",
      "epoch: 987, train loss: 0.229475\n",
      "epoch: 988, train loss: 0.229475\n",
      "epoch: 989, train loss: 0.229475\n",
      "epoch: 990, train loss: 0.229476\n",
      "epoch: 991, train loss: 0.229474\n",
      "epoch: 992, train loss: 0.229475\n",
      "epoch: 993, train loss: 0.229475\n",
      "epoch: 994, train loss: 0.229476\n",
      "epoch: 995, train loss: 0.229475\n",
      "epoch: 996, train loss: 0.229475\n",
      "epoch: 997, train loss: 0.229476\n",
      "epoch: 998, train loss: 0.229475\n",
      "epoch: 999, train loss: 0.229476\n",
      "epoch: 1000, train loss: 0.229475\n",
      "Saving NN to ./model_\n"
     ]
    }
   ],
   "source": [
    "train(1000,encoder,decoder,train_loader,noise_train_loader,loss_fn,optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8693, 23])\n",
      "torch.Size([4277, 23])\n"
     ]
    }
   ],
   "source": [
    "trainDataPath = \"./dataset/train_raw_preprocessed.csv\"\n",
    "tdf = pd.read_csv(trainDataPath)\n",
    "ty = tdf[\"Transported\"]\n",
    "tdf = tdf.drop(\"Transported\",axis=1)\n",
    "\n",
    "train_denoised = test(encoder,decoder,df=tdf)\n",
    "train_denoised[\"Transported\"] = ty\n",
    "train_denoised.to_csv(\"./dataset/train_denoised.csv\",index=False)\n",
    "testDataPath = \"./dataset/test_preprocessed.csv\"\n",
    "\n",
    "df = pd.read_csv(testDataPath)\n",
    "y = df[\"PassengerId\"]\n",
    "df = df.drop(\"PassengerId\",axis=1)\n",
    "denoised = test(encoder,decoder,df=df)\n",
    "denoised[\"PassengerId\"] = y\n",
    "denoised.to_csv(\"./dataset/test_denoised.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".Kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
